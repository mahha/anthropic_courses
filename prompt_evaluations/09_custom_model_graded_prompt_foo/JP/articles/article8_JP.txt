自然言語処理 (NLP) は、コンピューター サイエンスと人工知能の学際的なサブ分野です。それは主に、自然言語でエンコードされたデータを処理する能力をコンピュータに提供することに関係しており、情報検索、知識表現、および言語学のサブ分野である計算言語学と密接に関連しています。通常、データは、機械学習や深層学習におけるルールベース、統計ベース、またはニューラルベースのアプローチを使用して、テキスト コーパスとして収集されます。

自然言語処理の主なタスクは、音声認識、テキスト分類、自然言語理解、自然言語生成です。

歴史
詳細情報: 自然言語処理の歴史
自然言語処理のルーツは 1940 年代にあります。[1]すでに 1940 年に、アラン チューリングは「コンピューティング機械と知能」というタイトルの記事を発表し、現在ではチューリング テストと呼ばれるものを知能の基準として提案しましたが、当時は人工知能とは別の問題として明確にはされていませんでした。提案されたテストには、自然言語の自動解釈と生成を含むタスクが含まれています。

シンボリック NLP (1950 年代 – 1990 年代初頭)
シンボリック NLP の前提は、ジョン・サールの中国語の部屋の実験によくまとめられています。ルールのコレクション (たとえば、質問と対応する答えを含む中国語会話集) が与えられると、コンピューターは、それらのルールを直面するデータに適用することによって、自然言語理解 (またはその他の NLP タスク) をエミュレートします。

1950年代: 1954年のジョージタウン実験では、60以上のロシア語文を英語に完全に自動翻訳しました。著者らは、3 年か 5 年以内に機械翻訳の問題は解決されるだろうと主張しました [2]。しかし、実際の進歩ははるかに遅く、10 年間の研究が期待を満たせなかったことが判明した 1966 年の ALPAC 報告書以降、機械翻訳への資金は大幅に削減されました。最初の統計的機械翻訳システムが開発された 1980 年代後半まで、アメリカでは機械翻訳に関するさらなる研究はほとんど行われませんでした (ただし、日本やヨーロッパなどの他の地域でも一部の研究は続けられました[3])。
1960年代: 1960年代に開発され特に成功を収めた自然言語処理システムには、語彙が制限された制限された「ブロック世界」で動作する自然言語システムであるSHRDLUと、1964年から1966年にかけてジョゼフ・ワイゼンバウムによって書かれたロジェ派心理療法士のシミュレーションであるELIZAがありました。人間の思考や感情に関する情報をほとんど使用せず、ELIZAは時々驚くほど人間らしい対話を提供しました。 「患者」が非常に小さな知識ベースを超えた場合、ELIZA は一般的な応答を提供する可能性があります。たとえば、「頭が痛い」に対して「なぜ頭が痛いと言うのですか?」と応答する可能性があります。自然言語に関するロス・クイリアンの成功した研究は、わずか 20 語の語彙で実証されました。当時のコンピュータのメモリに収まるのはそれだけでしたからです [4]。
1970 年代: 1970 年代、多くのプログラマーは、現実世界の情報をコンピューターが理解できるデータに構造化する「概念的オントロジー」を書き始めました。例としては、MARGIE (Schank、1975)、SAM (Cullingford、1978)、PAM (Wilensky、1978)、TaleSpin (Meehan、1976)、QUALM (Lehnert、1977)、Politics (Carbonell、1979)、および Plot Units (Lehnert 1981) があります。この間に、最初のチャタボット (PARRY など) が作成されました。
1980 年代: 1980 年代から 1990 年代初頭は、NLP における記号手法の全盛期を迎えました。当時の重点分野には、ルールベースの構文解析 (例: 生成文法の計算操作化としての HPSG の開発)、形態論 (例: 2 レベル形態論 [5])、意味論 (例: Lesk アルゴリズム)、参照 (例: センタリング理論 [6] 内)、および自然言語理解のその他の領域 (例: 修辞構造理論) の研究が含まれていました。 Racter と Jabberwacky によるチャッターボットの開発など、他の研究分野も継続されました。重要な発展 (最終的に 1990 年代の統計的転換につながった) は、この時期に定量的評価の重要性が高まったことです。[7]
統計的 NLP (1990 年代～2010 年代)
1980 年代までは、ほとんどの自然言語処理システムは複雑な手書きのルールに基づいていました。しかし、1980 年代後半から、言語処理用の機械学習アルゴリズムの導入により、自然言語処理に革命が起こりました。これは、計算能力の着実な向上 (ムーアの法則を参照) と、チョムスキーの言語理論 (例: 変換文法) の優位性が徐々に弱まったことの両方によるものであり、その理論的基礎は、言語処理への機械学習アプローチの根底にある一種のコーパス言語学を妨げるものでした [8]。

1990 年代: NLP における統計手法における初期の顕著な成功の多くは、特に IBM アライメント モデルなどの IBM Research での研究により、機械翻訳の分野で起こりました。これらのシステムは、すべての政府手続きを対応する政府システムのすべての公用語に翻訳することを求める法律の結果として、カナダ議会と欧州連合によって作成された既存の多言語テキストコーパスを利用することができました。しかし、他のほとんどのシステムは、これらのシステムによって実装されるタスク用に特別に開発されたコーパスに依存しており、これがこれらのシステムの成功における大きな制限となっていました (そして多くの場合、現在もそうなり続けています)。その結果、限られた量のデータからより効果的に学習する方法について、多くの研究が行われてきました。
2000 年代: Web の成長に伴い、1990 年代半ば以降、生の (注釈のない) 言語データの量が増加して利用できるようになりました。したがって、研究では教師なし学習アルゴリズムと半教師あり学習アルゴリズムにますます焦点が当てられています。このようなアルゴリズムは、望ましい回答が手動で注釈付けされていないデータから、または注釈付きデータと注釈付けされていないデータの組み合わせを使用して学習できます。一般に、このタスクは教師あり学習よりもはるかに難しく、通常、特定の量の入力データに対して生成される結果の精度は低くなります。ただし、利用可能な注釈のないデータ (とりわけ、ワールド ワイド ウェブのコンテンツ全体を含む) が大量に存在するため、使用されるアルゴリズムの時間計算量が実用に十分低い場合には、劣った結果を補うことができることがよくあります。
ニューラル NLP (現在)
2003 年、当時最高の統計アルゴリズムであった単語 n グラム モデルは、Yoshua Bengio と共著者らによる多層パーセプトロン (単一の隠れ層と、言語モデリングの CPU クラスターで最大 1,400 万の単語で学習された複数の単語のコンテキスト長を持つ) の性能を上回りました。 [9]

2010 年に、Tomáš Mikolov (当時ブルノ工科大学の博士課程の学生) は共著者とともに、単一の隠れ層を持つ単純なリカレント ニューラル ネットワークを言語モデリングに適用し [10]、その後数年で Word2vec の開発を続けました。 2010 年代には、表現学習とディープ ニューラル ネットワーク スタイル (多くの隠れ層を特徴とする) の機械学習手法が自然言語処理で普及しました。その人気の一部は、そのような技術[11][12]が言語モデリング[13]や解析など、多くの自然言語タスクにおいて最先端の結果を達成できることを示す一連の結果によるものです。[14][15]これは医学やヘルスケアにおいてますます重要になっており、NLP はケアの改善 [16] や患者のプライバシーの保護を目指す際に、研究のためにアクセスできない電子医療記録のメモやテキストを分析するのに役立ちます [17]。

アプローチ: シンボリック、統計、ニューラル ネットワーク
記号的アプローチ、つまり、辞書検索と組み合わせて記号を操作するための一連のルールを手作業でコーディングすることは、歴史的には、AI 一般と特に NLP の両方で使用された最初のアプローチでした。[18][19]、文法を書いたり、ステミングのためのヒューリスティック ルールを考案したりするなどです。

一方、統計ネットワークとニューラル ネットワークの両方を含む機械学習アプローチには、記号アプローチに比べて多くの利点があります。

統計手法とニューラル ネットワーク手法はいずれも、テキストのコーパスから抽出された最も一般的なケースにより重点を置くことができますが、ルールベースのアプローチでは、まれなケースと一般的なケースの両方に均等にルールを提供する必要があります。
統計的またはニューラルネットワーク手法のいずれかによって生成される言語モデルは、やはり生成コストが高いルールベースのシステムと比較して、なじみのないもの（例：これまでに見たことのない単語や構造が含まれている）と誤った入力（例：スペルミスの単語や誤って省略された単語）の両方に対してより堅牢です。
このような (確率的) 言語モデルは大きくなるほど正確になります。これとは対照的に、ルールベースのシステムでは、ルールの量と複雑さを増やすことによってのみ精度が向上し、扱いにくい問題が生じます。
シンボルを操作するためのルールベースのシステムは 2020 年時点でもまだ使用されていましたが、2023 年には LLM の進歩によりほとんど廃止されました。

それ以前は、以下のものが一般的に使用されていました。

トレーニング データの量が、機械学習手法をうまく適用するには不十分な場合 (たとえば、Apertium システムによって提供されるような低リソース言語の機械翻訳の場合)、
NLP パイプラインでの前処理 (トークン化など)、または
NLP パイプラインの出力の後処理と変換用 (構文解析からの知識抽出など)。
統計的アプローチ
1980 年代後半から 1990 年代半ばにかけて、統計的アプローチにより、ルールベースのアプローチの非効率性によって引き起こされた AI の冬の時期に終止符が打たれました [20][21]。

ハード if-then ルールのシステムを生成する初期のデシジョン ツリーは、依然として古いルールベースのアプローチに非常に似ていました。古いルールベースのアプローチの終焉を告げたのは、品詞タグ付けに適用された隠れマルコフ モデルの導入だけでした。

ニューラルネットワーク
詳細情報: 人工ニューラル ネットワーク
統計的手法の大きな欠点は、精緻な特徴量エンジニアリングが必要になることです。 2015 年以降 [22]、統計的アプローチはニューラル ネットワーク アプローチに置き換えられ、セマンティック ネットワーク [23] と単語の埋め込みを使用して単語の意味的特性を捕らえるようになりました。

中間タスク (品詞のタグ付けや依存関係の解析など) はもう必要ありません。

当時新しく発明された配列間変換に基づくニューラル機械翻訳により、以前は統計的機械翻訳に必要であった単語のアライメントなどの中間ステップが廃止されました。

一般的な NLP タスク
以下は、自然言語処理で最も一般的に研究されているタスクの一部のリストです。これらのタスクの中には、現実世界に直接応用できるものもありますが、他のタスクはより一般的に、より大きなタスクの解決を支援するために使用されるサブタスクとして機能します。

自然言語処理タスクは密接に絡み合っていますが、便宜上いくつかのカテゴリに分けることができます。大まかな分割は以下の通りです。

テキストと音声の処理
光学式文字認識 (OCR)
印刷されたテキストを表す画像が与えられた場合、対応するテキストを決定します。
音声認識
一人または複数の人々が話しているサウンド クリップが与えられた場合、そのスピーチのテキスト表現を決定します。これはテキスト読み上げの逆であり、口語的に「AI 完全」と呼ばれる非常に難しい問題の 1 つです (上記を参照)。自然な音声では、連続する単語の間に休止期間がほとんどないため、音声の分割は音声認識の必要なサブタスクです (下記を参照)。ほとんどの話し言葉では、連続する文字を表す音が調音と呼ばれるプロセスで互いに混ざり合うため、アナログ信号を個別の文字に変換するのは非常に困難なプロセスになる可能性があります。また、同じ言語の単語が異なるアクセントを持つ人々によって話されることを考えると、音声認識ソフトウェアは、テキスト上の等価性という観点から、さまざまな入力を互いに同一であるものとして認識できなければなりません。
音声セグメンテーション
1 人または複数の人々が話しているサウンド クリップが与えられた場合、それを単語に分割します。音声認識のサブタスクであり、通常は音声認識とグループ化されます。
テキスト読み上げ
テキストが与えられると、それらの単位を変換して音声表現を生成します。テキスト読み上げは、視覚障害者を支援するために使用できます。[24]
単語の分割 (トークン化)
トークン化は、テキストを個々の単語または単語の断片に分割するテキスト分析で使用されるプロセスです。この手法により、単語インデックスとトークン化されたテキストという 2 つの重要なコンポーネントが生成されます。単語インデックスは、一意の単語を特定の数値識別子にマップするリストであり、トークン化されたテキストは各単語を対応する数値トークンに置き換えます。これらの数値トークンは、さまざまな深層学習手法で使用されます。[25]
英語のような言語の場合、単語は通常スペースで区切られるため、これはかなり簡単です。ただし、中国語、日本語、タイ語などの一部の書き言葉では、そのような方法で単語の境界をマークしません。これらの言語では、テキストの分割は、言語内の単語の語彙と形態に関する知識を必要とする重要な作業です。このプロセスは、データ マイニングにおけるバッグ オブ ワード (BOW) の作成などの場合にも使用されることがあります。[要出典]
形態学的解析
見出し語化
屈折語尾のみを削除し、補題とも呼ばれる単語の基本辞書形式を返すタスク。見出し語化は、単語を正規化された形式に変換するためのもう 1 つのテクニックです。しかし、この場合、変換では実際に辞書を使用して単語を実際の形式にマッピングします。[26]
形態学的セグメンテーション
単語を個々の形態素に分割し、形態素のクラスを識別します。このタスクの難易度は、対象となる言語の形態 (つまり、単語の構造) の複雑さに大きく依存します。英語はかなり単純な形態論、特に屈折形態論を持っているため、多くの場合、このタスクを完全に無視して、単純に単語のすべての可能な形式 (例: 「open、opens、opened、opening」) を個別の単語としてモデル化することが可能です。しかし、トルコ語や凝集性の高いインドの言語であるメイテイ語などの言語では、辞書の各項目に何千もの可能な語形があるため、このようなアプローチは不可能です。[27]
品詞タグ付け
与えられた文で、各単語の品詞 (POS) を決定します。多くの単語、特に一般的な単語は、複数の品詞として機能します。たとえば、「book」は名詞 (「テーブルの上の本」) または動詞 (「飛行機を予約する」) の場合があります。 「set」には名詞、動詞、または形容詞を使用できます。 「out」は、少なくとも 5 つの異なる品詞のいずれかになります。
ステミング
語形変化した (場合によっては派生した) 単語を基本形に減らすプロセス (たとえば、「close」は、「closed」、「closed」、「close」、「closer」などの語根になります)。ステミングは見出し語化と同様の結果をもたらしますが、辞書ではなくルールに基づいて行われます。
構文解析
シリーズの一部
形式言語
主要な概念
形式体系アルファベット構文意味論（論理）意味論（プログラミング言語）形式文法形成規則整形式式オートマトン理論正規表現生産根拠表現原子式
アプリケーション
vte
文法導入[28]
言語の構文を記述する正式な文法を生成します。
文分割 (「文境界の曖昧さ回避」とも呼ばれます)
テキストの塊が与えられた場合、文の境界を見つけます。文の境界はピリオドやその他の句読点でマークされることがよくありますが、これらの同じ文字が他の目的 (たとえば、略語のマーク) に使用される場合があります。
解析中
与えられた文の解析ツリー (文法分析) を決定します。自然言語の文法は曖昧で、典型的な文には複数の可能な解析があります。おそらく驚くべきことに、典型的な文には何千もの潜在的な解析が存在する可能性があります (人間にとってそのほとんどは完全に無意味に見えるでしょう)。解析には、依存関係の解析と構成要素の解析という 2 つの主なタイプがあります。依存関係解析は文内の単語間の関係 (主目的語や述語などのマーク付け) に焦点を当てますが、構成要素解析は確率的文脈自由文法 (PCFG) を使用して解析ツリーを構築することに焦点を当てます (確率的文法も参照)。
語彙意味論 (文脈内の個々の単語の)
語彙意味論
コンテキスト内の個々の単語の計算上の意味は何ですか?
分布セマンティクス
データから意味表現を学ぶにはどうすればよいでしょうか?
固有表現認識 (NER)
テキストのストリームが与えられた場合、テキスト内のどの項目が人や場所などの固有名に対応しているか、またそのようなそれぞれの名前の種類 (人、場所、組織など) を判断します。大文字化は英語などの言語で固有表現を認識するのに役立ちますが、この情報は固有表現の種類を決定するのには役立ちません。また、いずれの場合も不正確または不十分であることがよくあります。たとえば、文の最初の文字も大文字になり、名前付きエンティティは複数の単語にまたがることが多く、そのうちの一部のみが大文字になります。さらに、非西洋文字の他の多くの言語 (中国語やアラビア語など) には大文字化がまったくなく、大文字化がある言語であっても名前を区別するために一貫して大文字を使用していない可能性があります。たとえば、ドイツ語では、名前であるかどうかに関係なく、すべての名詞が大文字になります。また、フランス語とスペイン語では、形容詞として機能する名前は大文字になりません。このタスクの別名はトークン分類です。[29]
感情分析 (マルチモーダル感情分析も参照)
感情分析は、テキストの背後にある感情的な意図を特定して分類するために使用される計算手法です。この手法には、テキストを分析して、表現された感情が肯定的、否定的、または中立的であるかを判断することが含まれます。感情分類のモデルは通常、単語 N グラム、用語頻度 - 逆文書頻度 (TF-IDF) 特徴、手動生成特徴などの入力を利用するか、テキスト シーケンス内の長期依存関係と短期依存関係の両方を認識するように設計された深層学習モデルを採用します。センチメント分析の応用は多岐にわたり、さまざまなオンライン プラットフォームでの顧客レビューの分類などのタスクにまで及びます。
用語の抽出
用語抽出の目的は、指定されたコーパスから関連する用語を自動的に抽出することです。
語義の曖昧さ回避 (WSD)
多くの単語には複数の意味があります。文脈の中で最も意味のある意味を選択する必要があります。この問題では、通常、単語とそれに関連する単語の意味のリストが与えられます。辞書や WordNet などのオンライン リソースから。
エンティティのリンク
多くの単語 (通常は固有名詞) は名前付き実体を指します。ここでは、コンテキスト内で参照されるエンティティ (有名な個人、場所、会社など) を選択する必要があります。
関係意味論 (個々の文の意味論)
関係性の抽出
テキストの塊が与えられた場合、名前付きエンティティ間の関係 (例: 誰が誰と結婚しているか) を特定します。
セマンティック解析
テキストの一部 (通常は文) が与えられると、グラフとして (AMR 解析など)、または論理形式に従って (DRT 解析など)、そのセマンティクスの正式な表現を生成します。この課題には通常、意味論からのさらにいくつかの初歩的な NLP タスクの側面 (例: 意味役割のラベル付け、語義の曖昧さの除去) が含まれており、本格的な談話分析 (例: 談話分析、共参照。以下の自然言語理解を参照) を含むように拡張することができます。
意味的役割のラベル付け (以下の暗黙的な意味的役割のラベル付けも参照)
単一の文が与えられた場合、意味論的な述語 (例: 言葉のフレーム) を特定して曖昧さを解消し、次にフレーム要素 (意味論的な役割) を特定して分類します。
談話 (個々の文を超えた意味論)
相互参照の解決
文または大きなテキストの塊が与えられた場合、どの単語 (「言及」) が同じオブジェクト (「エンティティー」) を参照しているかを判別します。照応解決はこのタスクの具体的な例であり、特に代名詞とそれが参照する名詞または名前を一致させることに関係します。共参照解決のより一般的なタスクには、参照式を含むいわゆる「ブリッジ関係」を識別することも含まれます。たとえば、「彼は玄関からジョンの家に入った」のような文では、「玄関」は指示表現であり、識別されるべき橋渡し関係は、参照されているドアが（同様に参照される可能性のある他の構造物の玄関ではなく）ジョンの家の玄関ドアであるという事実である。
談話分析
このルーブリックには、いくつかの関連タスクが含まれています。タスクの 1 つは談話解析です。つまり、接続されたテキストの談話構造、つまり文間の談話関係の性質 (例: 詳細、説明、対比) を識別します。考えられるもう 1 つのタスクは、テキストの塊内の音声行為 (例: はい/いいえの質問、内容の質問、発言、主張など) を認識して分類することです。
暗黙的なセマンティックな役割のラベル付け
単一の文が与えられた場合、意味論的述語 (例: 言葉フレーム) と現在の文におけるそれらの明示的な意味論的役割を識別して曖昧さを解消します (上記の意味論的役割のラベル付けを参照)。次に、現在の文内で明示的に実現されていない意味上の役割を特定し、それらをテキスト内の他の場所で明示的に実現されている引数と指定されていない引数に分類し、前者をローカル テキストに対して解決します。密接に関連するタスクは、ゼロ照応解決、つまり、ドロップ支持言語への共参照解決の拡張です。
テキストの含意の認識
2 つのテキスト断片が与えられた場合、一方が true であることが他方を伴うか、他方の否定を伴うか、あるいは他方が true または false であることを許容するかを判断します。[30]
トピックの分割と認識
与えられたテキストの塊を、トピックに特化したセグメントに分割し、セグメントのトピックを特定します。
引数マイニング
引数マイニングの目標は、コンピュータ プログラムを利用して自然言語テキストから引数構造を自動的に抽出および識別することです。[31]このような議論の構造には、前提、結論、議論のスキーム、および談話内の主な議論と副次的な議論、または主な議論と反論の間の関係が含まれます[32][33]。
より高いレベルの NLP アプリケーション
自動要約（テキスト要約）
テキストの塊の読みやすい要約を作成します。研究論文や新聞の財務セクションの記事など、既知の種類のテキストの要約を提供するためによく使用されます。
文法上の誤りの修正
文法エラーの検出と修正には、言語分析のあらゆるレベル (音韻論/正書法、形態論、構文、意味論、語用論) に関する広範な問題が含まれます。文法的誤りの修正は、英語を第二言語として使用または習得する何億人もの人々に影響を与えるため、大きな影響を及ぼします。したがって、2011年以来、多くの共有タスクの対象となっている[34][35][36]。正書法、形態論、構文、および意味論の特定の側面に関する限り、GPT-2 などの強力なニューラル言語モデルの開発により、これは現在 (2019 年) ほぼ解決された問題とみなされ、さまざまな商用アプリケーションで販売されています。
論理変換
テキストを自然言語から形式ロジックに変換します。
機械翻訳（MT）
テキストをある人間の言語から別の言語に自動的に翻訳します。これは最も難しい問題の 1 つであり、口語的に「AI 完全」と呼ばれる問題のクラスのメンバーです。つまり、適切に解決するには、人間が持つさまざまな種類の知識 (文法、意味論、現実世界に関する事実など) をすべて必要とします。
自然言語理解 (NLU)
テキストのチャンクを、コンピュータ プログラムが操作しやすい一次論理構造などのより正式な表現に変換します。自然言語理解には、通常、自然言語概念の組織化された表記の形式をとる自然言語表現から導き出すことができる複数の可能な意味論から、意図された意味論を特定することが含まれます。言語メタモデルとオントロジーの導入と作成は効率的ではありますが、経験に基づく解決策です。閉世界仮定 (CWA) とオープンワールドの仮定、または主観的な Yes/No と客観的な True/False は、意味論的形式化の基礎の構築に期待されています。[37]
自然言語生成 (NLG):
コンピュータ データベースまたはセマンティック インテントからの情報を、可読な人間の言語に変換します。
本の生成
本来の NLP タスクではありませんが、自然言語生成やその他の NLP タスクの拡張として、本格的な書籍の作成が行われます。最初の機械生成本は 1984 年にルールベースのシステムによって作成されました (ラクター、警察官のひげは半分構築されています) [38]。ニューラル ネットワークによる最初の出版作品は 2018 年に出版され、小説として販売された 1 the Road には 6,000 万語が含まれています。これらのシステムはどちらも、基本的には精巧ではありますが、意味をなさない (意味論のない) 言語モデルです。最初の機械生成科学書は 2019 年に出版されました (Beta Writer、Lithium-Ion Batteries、Springer、Cham) [39]。 『Racter and 1 the Road』とは異なり、これは事実の知識に基づいており、テキストの要約に基づいています。
ドキュメントAI
Document AI プラットフォームは NLP テクノロジーの上に位置し、人工知能、機械学習、または NLP の経験のないユーザーでも、さまざまな種類のドキュメントから必要な特定のデータを抽出するようにコンピューターを迅速にトレーニングできます。 NLP を活用した Document AI により、弁護士、ビジネス アナリスト、会計士など、非技術チームがドキュメントに隠された情報に迅速にアクセスできるようになります。[40]
対話管理
人間と会話することを目的としたコンピュータ システム。
質疑応答
人間の言語で質問が与えられ、その答えを判断します。一般的な質問には特定の正解があります (「カナダの首都はどこですか?」など) が、自由形式の質問も考慮される場合があります (「人生の意味は何ですか?」など)。
テキストから画像への生成
画像の説明が与えられた場合、その説明に一致する画像を生成します。[41]
テキストからシーンへの生成
シーンの説明が与えられたら、そのシーンの 3D モデルを生成します。[42][43]
テキストからビデオへ
ビデオの説明が与えられた場合、その説明に一致するビデオを生成します。[44][45]
一般的な傾向と（考えられる）将来の方向性
この分野における長年の傾向に基づいて、NLP の将来の方向性を推定することが可能です。 2020 年の時点で、長期にわたる CoNLL 共有タスク シリーズのトピックには 3 つの傾向が見られます。[46]

自然言語のますます抽象的で「認知的」な側面への関心 (1999 ～ 2001 年: 浅い解析、2002 ～ 2003 年: 固有表現認識、2006 ～ 09/2017 ～ 18 年: 依存関係構文、2004 ～ 05/2008 ～ 2009 年意味論的役割ラベル付け、2011 ～ 12 年相互参照、2015 ～ 16 年: 談話解析、 2019: セマンティック解析)。
多言語、および潜在的にマルチモダリティへの関心の高まり（1999年から英語、2002年からスペイン語、オランダ語、2003年からドイツ語、2006年からブルガリア語、デンマーク語、日本語、ポルトガル語、スロベニア語、スウェーデン語、トルコ語、2007年からバスク語、カタロニア語、中国語、ギリシャ語、ハンガリー語、イタリア語、トルコ語、2007年からチェコ語） 2009 年、2012 年以降はアラビア語、2017 年は 40 言語以上、2018 年は 60 言語以上、100 言語以上
シンボリック表現の排除（ルールベースの教師ありから弱教師ありの手法、表現学習、エンドツーエンドシステム）
認知
ほとんどの高レベルの NLP アプリケーションには、インテリジェントな動作と自然言語の見かけの理解をエミュレートする側面が含まれています。より広く言えば、認知行動のますます高度な側面の技術的運用化は、NLP の発展軌道の 1 つを表しています (上記の CoNLL 共有タスクの傾向を参照)。

認知とは、「思考、経験、感覚を通じて知識や理解を獲得する精神的な作用またはプロセス」を指します[47]。認知科学は、心とそのプロセスに関する学際的で科学的な研究です[48]。認知言語学は言語学の学際的な分野であり、心理学と言語学の両方の知識と研究を組み合わせたものです[49]。特に記号 NLP の時代には、計算言語学の分野は認知研究との強いつながりを維持していました。

一例として、George Lakoff は、認知言語学の知見に加えて、認知科学の観点から自然言語処理 (NLP) アルゴリズムを構築する方法論を提供しています。次の 2 つの特徴があります。

レイコフによって「あるアイデアを別のアイデアの観点から理解すること」として説明されている概念的メタファーの理論を適用すると、作者の意図がわかります。 [51]たとえば、英語の「big」という単語について考えてみましょう。比較で使用される場合 (「あれは大きな木です」)、著者の意図は、その木が他の木や著者の経験と比較して物理的に大きいことを暗示することです。比喩的に使用される場合 (「明日は大事な日だ」)、重要性を暗示するという作者の意図。 「彼女は大きな人です」など、他の使用法の背後にある意図は、追加情報がなければ、人にとっても認知的 NLP アルゴリズムにとっても同様にやや曖昧なままになります。
確率的文脈自由文法 (PCFG) などを使用して、分析対象のテキストの前後に提示された情報に基づいて、単語、フレーズ、文、またはテキストの一部に意味の相対的な尺度を割り当てます。このようなアルゴリズムの数学的方程式は、米国特許 9269353 に示されています。[52]
R
M
M
(
t
ああ
k
e
n
N
)
=
P
M
M
(
t
ああ
k
e
n
N
)
×
1
2
d
(
∑
私は
=
−
d
d
(
(
P
M
M
(
t
ああ
k
e
n
N
)
×
P
F
(
t
ああ
k
e
n
N
−
私は
、
t
ああ
k
e
n
N
、
t
ああ
k
e
n
N
+
私は
)
)
私は
)
{\displaystyle {RMM(token_{N})}={PMM(token_{N})}\times {\frac {1}{2d}}\left(\sum _{i=-d}^{d}{((PMM(token_{N})}\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\right)}
どこで
RMM は意味の相対的な尺度です
トークンは、テキスト、文、語句、または単語の任意のブロックです。
N は分析されるトークンの数です
PMM はコーパスに基づいた意味の確率的な尺度です
d は、N 個のトークンのシーケンスに沿ったトークンのゼロ以外の位置です。
PF は言語に固有の確率関数です
認知言語学とのつながりは NLP の歴史的遺産の一部ですが、1990 年代の統計上の転換以来、取り上げられることは少なくなりました。それにもかかわらず、技術的に運用可能なフレームワークに向けた認知モデルを開発するアプローチは、認知文法[53]、機能文法[54]、構築文法[55]、計算心理言語学、認知神経科学（ACT-Rなど）などのさまざまなフレームワークの文脈で追求されてきたが、主流のNLPでの取り入れは限られていた（ACLの主要な会議[56]への出席によって測定される）。最近では、コグニティブ NLP のアイデアが、説明可能性を達成するためのアプローチとして、たとえば「コグニティブ AI」の概念の下で復活しています。[57]同様に、認知 NLP のアイデアは、ニューラル モデルのマルチモーダル NLP (めったに明示されません)[58]、人工知能の開発、特に大規模言語モデル アプローチを使用するツールと技術 [59]、および英国の神経科学者でありユニバーシティ カレッジ ロンドンの理論家であるカール J. フリストンによる自由エネルギー原理に基づく一般人工知能の新しい方向性に固有のものです。