自然言語処理 (NLP) では、単語の埋め込みは単語の表現です。埋め込みはテキスト分析に使用されます。通常、表現は、ベクトル空間内でより近い単語の意味が類似していると予想されるような方法で単語の意味をエンコードする実数値ベクトルです。[1]単語の埋め込みは、言語モデリングと特徴学習技術を使用して取得でき、語彙の単語またはフレーズが実数のベクトルにマッピングされます。

このマッピングを生成する方法には、ニューラル ネットワーク[2]、単語共起行列の次元削減[3][4][5]、確率モデル[6]、説明可能な知識ベース法[7]、および単語が出現するコンテキストの観点からの明示的表現[8]が含まれます。

単語やフレーズの埋め込みを基礎となる入力表現として使用すると、構文解析[9]や感情分析などの NLP タスクのパフォーマンスが向上することが示されています。

アプローチの開発と歴史
分布意味論では、観察された言語、単語埋め込み、または意味論的特徴空間モデルの意味を理解するための定量的方法論的アプローチが、知識表現としてしばらく使用されてきました [11]。このようなモデルは、言語データの大規模サンプルにおける分布特性に基づいて、言語項目間の意味論的な類似性を定量化および分類することを目的としています。 「単語は、それが保持する仲間によって特徴づけられる」という根本的な考え方は、ジョン・ルパート・ファースによる 1957 年の論文で提案されました [12] が、検索システムに関する同時代の研究 [13] や認知心理学にもルーツがあります [14]。

ベクトルまたは埋め込みとして表現される語彙項目 (単語または複数単語の用語) を含む意味空間の概念は、分布特性を捕捉し、それらを実際のアプリケーションに使用して単語、語句、または文書全体の類似性を測定するという計算上の課題に基づいています。意味論的空間モデルの第一世代は、情報検索用のベクトル空間モデルです。[15][16][17]最も単純な形で実装された単語とその分布データのこのようなベクトル空間モデルは、高次元の非常にまばらなベクトル空間をもたらします (次元の呪いを参照)。特異値分解などの線形代数手法を使用して次元数を削減したことにより、1980 年代後半の潜在意味分析と、単語の共起コンテキストを収集するためのランダム インデックス アプローチの導入につながりました。[18][19][20][21] 2000 年に、Bengio ら。 「神経確率言語モデル」というタイトルの一連の論文で提供され、「単語の分散表現を学習する」ことで文脈内の単語表現の高次元性を削減します。[22][23][24]

NeurIPS (NIPS) 2002 で発表された研究では、カーネル CCA の方法を二言語 (および多言語) コーパスに適用する単語と文書の両方の埋め込みの使用が紹介され、単語埋め込みの自己教師あり学習の初期の例も提供されました [25]

単語の埋め込みには 2 つの異なるスタイルがあります。1 つは単語を共起単語のベクトルとして表現するスタイル、もう 1 つは単語をその単語が出現する言語コンテキストのベクトルとして表現するスタイルです。これらの異なるスタイルは、Lavelli et al.、2004 で研究されています。[26] Roweis と Saul は、高次元データ構造の表現を発見するために「局所線形埋め込み」(LLE) を使用する方法を Science 誌に発表しました。 2005 年頃以降のほとんどの新しい単語埋め込み技術は、Yoshua Bengio[28][循環参照] と同僚によって行われた基礎研究の後、より確率的および代数的なモデルではなく、ニューラル ネットワーク アーキテクチャに依存しています。

このアプローチは、2010 年にベクトルの品質とモデルのトレーニング速度に関する理論的進歩が見られた後、またハードウェアの進歩により、より広いパラメーター空間を有益に探索できるようになった後、多くの研究グループによって採用されてきました。 2013 年、Tomas Mikolov 率いる Google のチームは、これまでのアプローチよりも高速にベクトル空間モデルをトレーニングできる単語埋め込みツールキットである word2vec を作成しました。 word2vec アプローチは実験で広く使用されており、技術としての単語埋め込みへの関心を高めるのに役立ち、研究の流れを専門的な研究からより広範な実験に移し、最終的には実用化への道を切り開きました。

多義性と同音異義語
歴史的に、静的な単語埋め込みまたは単語ベクトル空間モデルの主な制限の 1 つは、複数の意味を持つ単語が 1 つの表現 (意味空間内の 1 つのベクトル) にまとめられてしまうことです。つまり、多義性と同音異義語が適切に処理されていません。たとえば、「昨日試したクラブは素晴らしかったです!」という文では、クラブという用語が、クラブサンドイッチ、クラブハウス、ゴルフクラブ、またはクラブが持つその他の意味と関連しているかどうかは明らかではありません。単語ごとに複数の意味を異なるベクトル (多意味埋め込み) で収容する必要性が、NLP におけるいくつかの貢献で単一意味埋め込みを多意味埋め込みに分割する動機となっています [32] [33]。

多意味埋め込みを生成するほとんどのアプローチは、語義表現に関して 2 つの主なカテゴリ、つまり教師なしと知識ベースに分類できます。[34] word2vec スキップグラムに基づいて、マルチセンス スキップグラム (MSSG)[35] は、単語ごとに特定の数の意味を想定しながら、単語の意味の識別と埋め込みを同時に実行し、トレーニング時間を短縮します。ノンパラメトリック マルチセンス スキップグラム (NP-MSSG) では、この数は各単語に応じて変化します。語彙データベース (WordNet、ConceptNet、BabelNet など)、単語の埋め込み、および語義の曖昧さ回避に関する事前知識を組み合わせた最適なセンス アノテーション (MSSA)[36] は、事前定義されたスライディング ウィンドウで単語のコンテキストを考慮しながら、教師なしの知識ベースのアプローチを通じて語義にラベルを付けます。単語の曖昧さが解消されると、標準的な単語埋め込み手法で使用できるため、多意味埋め込みが生成されます。 MSSA アーキテクチャにより、曖昧さの除去と注釈のプロセスを自己改善的な方法で繰り返し実行できます。[37]

マルチセンス埋め込みの使用は、品詞タグ付け、意味関係識別、意味関連性、固有表現認識、感情分析など、いくつかの NLP タスクのパフォーマンスを向上させることが知られています。[38][39]

2010 年代後半の時点では、ELMo や BERT などの文脈的に意味のある埋め込みが開発されています [40]。静的な単語の埋め込みとは異なり、これらの埋め込みはトークン レベルであり、単語の出現ごとに独自の埋め込みが存在します。これらの埋め込みは、単語の多意味の性質をよりよく反映しています。これは、類似した文脈での単語の出現が BERT の埋め込み空間の同様の領域に位置するためです。[41][42]

生物学的配列の場合: BioVectors
バイオインフォマティクス応用のための生物学的配列 (DNA、RNA、タンパク質など) における n グラムの単語埋め込みは、Asgari と Mofrad によって提案されています。タンパク質（アミノ酸配列）の場合はタンパク質ベクター（ProtVec）、遺伝子配列の場合は遺伝子ベクター（GeneVec）とともに生物学的配列一般を指すためにバイオベクター（BioVec）と名付けられ、この表現はプロテオミクスおよびゲノミクスにおける深層学習のアプリケーションで広く使用できます。 Asgari と Mofrad によって提示された結果 [43] は、BioVector が基礎となるパターンの生化学的および生物物理学的解釈の観点から生物学的配列を特徴付けることができることを示唆しています。

ゲームデザイン
ゲーム デザインにおけるアプリケーションを使用した Word 埋め込みは、ゲームプレイ データのログを使用して緊急のゲームプレイを発見する方法として、Rabii と Cook によって提案されています[44]。このプロセスでは、ゲーム中に発生するアクションを形式言語で転写し、その結果得られたテキストを使用して単語の埋め込みを作成する必要があります。 Rabii と Cook[44] によって提示された結果は、結果として得られるベクトルが、ゲームのルールに明示的に記載されていないチェスのようなゲームに関する専門知識を捕捉できることを示唆しています。

文の埋め込み
詳細は「文の埋め込み」を参照
このアイデアは、文全体やドキュメントの埋め込みにも拡張されました。思考ベクトルの概念の形で。 2015 年に、一部の研究者は、機械翻訳の品質を向上させる手段として「スキップ思考ベクトル」を提案しました [45]。文を表現するためのより最近の一般的なアプローチは、Sentence-BERT (SentenceTransformers) です。これは、シャムおよびトリプレット ネットワーク構造を使用して事前トレーニングされた BERT を変更します。[46]

ソフトウェア
単語埋め込みをトレーニングおよび使用するためのソフトウェアには、Tomáš Mikolov の Word2vec、スタンフォード大学の GloVe、[47] GN-GloVe、[48] Flair embeddings、[38] AllenNLP の ELMo、[49] BERT、[50] fastText、Gensim、[51] Indra、[52]、および Deeplearning4j などがあります。主成分分析 (PCA) と T 分布確率的近傍埋め込み (t-SNE) は両方とも、単語ベクトル空間の次元を削減し、単語埋め込みとクラスターを視覚化するために使用されます。 [53]

応用例
たとえば、fastText は、オンラインで入手可能な Sketch Engine のテキスト コーパスの単語埋め込みを計算するためにも使用されます。[54]

倫理的影響
Bolukbasi et al. が示すように、単語の埋め込みには、トレーニングされたデータセットに含まれるバイアスやステレオタイプが含まれる場合があります。 2016年の論文「Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings」の中で、プロのジャーナリストによって書かれたテキストで構成されるGoogleニュースのテキスト（一般的に使用されるデータコーパス）でトレーニングされた、一般に利用可能な（そして人気のある）word2vec埋め込みが、依然として単語の類推を抽出する際に、性別と人種の偏見を反映した不均衡な単語の関連付けを示していることを指摘しています。[55]たとえば、前述の単語の埋め込みを使用して生成されたアナロジーの 1 つは、「男性はコンピューター プログラマーであり、女性は主婦である」です。[56][57]

Jieyu Zhou らによって行われた研究。これらの訓練された単語埋め込みを注意深く監視せずに適用すると、社会に存在する偏見が永続化する可能性が高く、その偏見は未変更の訓練データを通じて導入されることを示しています。さらに、単語の埋め込みによってこれらのバイアスが増幅されることさえあります。[58][59]