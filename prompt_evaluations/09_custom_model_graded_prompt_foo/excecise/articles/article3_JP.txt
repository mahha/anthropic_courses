機械学習において、バックプロパゲーションは、ネットワーク パラメーターの更新を計算するためにニューラル ネットワークをトレーニングするために一般的に使用される勾配推定方法です。

これは、連鎖ルールをニューラル ネットワークに効率的に適用したものです。バックプロパゲーションは、単一の入出力例のネットワークの重みに関する損失関数の勾配を計算します。これを効率的に実行し、一度に 1 層ずつ勾配を計算し、最後の層から逆方向に反復して、連鎖ルール内の中間項の冗長な計算を回避します。これは動的プログラミングを通じて導き出すことができます。[1][2][3]

厳密に言うと、バックプロパゲーションという用語は、勾配を効率的に計算するためのアルゴリズムのみを指し、勾配の使用方法を指すものではありません。しかし、この用語は、確率的勾配降下法などによる勾配の使用方法や、Adam などのより複雑なオプティマイザの中間ステップとしての勾配の使用方法を含む、学習アルゴリズム全体を指すために大まかに使用されることがよくあります[4]。

バックプロパゲーションには複数の発見と部分的な発見があり、歴史と用語が複雑に絡み合っていました。詳細については、歴史のセクションを参照してください。このテクニックの他の名前には、「自動微分の逆モード」または「逆累積」などがあります。[5]

概要
バックプロパゲーションは、損失関数に関して、フィードフォワード ニューラル ネットワークの重み空間の勾配を計算します。以下を示します:

×
{\displaystyle x}: 入力 (特徴のベクトル)
y
{\displaystyle y}: ターゲット出力
分類の場合、出力はクラス確率のベクトルになります (例: 
(
0.1
、
0.7
、
0.2
)
{\displaystyle (0.1,0.7,0.2)}、ターゲット出力は、ワンホット/ダミー変数によってエンコードされた特定のクラスです (例: 
(
0
、
1
、
0
)
{\displaystyle (0,1,0)})。
C
{\displaystyle C}: 損失関数または「コスト関数」[a]
分類の場合、これは通常クロスエントロピー (XC、対数損失) ですが、回帰の場合、通常は二乗誤差損失 (SEL) です。
L
{\displaystyle L}: レイヤーの数
W
私
=
(
w
j
k
私
)
{\displaystyle W^{l}=(w_{jk}^{l})}: レイヤー間の重み 
私
−
1
{\displaystyle l-1} と 
私
{\displaystyle l}、ここで 
w
j
k
私
{\displaystyle w_{jk}^{l}} は、 
k
層内の {\displaystyle k} 番目のノード 
私
−
1
{\displaystyle l-1} と 
j
レイヤー内の {\displaystyle j} 番目のノード 
私
{\displaystyle l}[b]
f
私
{\displaystyle f^{l}}: 層の活性化関数 
私
{\displaystyle l}
分類の場合、最後の層は通常、二値分類の場合はロジスティック関数、多クラス分類の場合はソフトマックス (softargmax) です。一方、隠れ層の場合、これは伝統的に各ノード (座標) 上のシグモイド関数 (ロジスティック関数またはその他) でしたが、現在はより多様になり、整流器 (ランプ、ReLU) が一般的です。
ある
j
私
{\displaystyle a_{j}^{l}}: のアクティブ化 
j
レイヤー内の {\displaystyle j} 番目のノード 
私
{\displaystyle l}。
バックプロパゲーションの導出では、以下で必要に応じてその他の中間量を導入して使用します。バイアス項は、固定入力 1 の重みに対応するため、特別に扱われません。バックプロパゲーションの場合、特定の損失関数と活性化関数は、それらとその導関数が効率的に評価できる限り重要ではありません。従来の活性化関数には、シグモイド、tanh、ReLU などがあります。その後、swish[6]、mish、[7]、およびその他の活性化関数も同様に提案されています。

ネットワーク全体は、関数合成と行列乗算の組み合わせです。

g
(
×
)
:=
f
L
(
W
L
f
L
−
1
(
W
L
−
1
⋯
f
1
(
W
1
×
)
⋯
)
)
{\displaystyle g(x):=f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{1}(W^{1}x)\cdots ))}
トレーニング セットには、一連の入力と出力のペアが存在します。 
{
(
×
私は
、
y
私は
)
}
{\displaystyle \left\{(x_{i},y_{i})\right\}}。各入出力ペアについて 
(
×
私は
、
y
私は
)
トレーニング セット内の {\displaystyle (x_{i},y_{i})} の場合、そのペアでのモデルの損失は、予測された出力の差のコストとなります。 
g
(
×
私は
)
{\displaystyle g(x_{i})} とターゲット出力 
y
私は
{\displaystyle y_{i}}:

C
(
y
私は
、
g
(
×
私は
)
)
{\displaystyle C(y_{i},g(x_{i}))}
違いに注意してください。モデルの評価中、重みは固定されていますが、入力は変化します (ターゲットの出力は不明な場合があります)。ネットワークは出力層で終わります (損失関数は含まれません)。モデルのトレーニング中、入出力ペアは固定されますが、重みは変化し、ネットワークは損失関数で終了します。

バックプロパゲーションは、固定の入出力ペアの勾配を計算します。 
(
×
私は
、
y
私は
)
{\displaystyle (x_{i},y_{i})}、ここでの重みは 
w
j
k
私
{\displaystyle w_{jk}^{l}} は異なる場合があります。グラデーションの個々のコンポーネント、 
∂
C
/
∂
w
j
k
私
、
{\displaystyle \partial C/\partial w_{jk}^{l},} は連鎖規則によって計算できます。ただし、これを重みごとに個別に行うのは非効率です。バックプロパゲーションは、各層の勾配 (特に、各層の重み付けされた入力の勾配) を計算することにより、重複した計算を回避し、不必要な中間値を計算しないことにより、勾配を効率的に計算します。 
δ
私
{\displaystyle \delta ^{l}} – 後ろから前へ。

非公式ですが、重要な点は、重みを付ける唯一の方法であるため、 
W
私
{\displaystyle W^{l}} は次の層への影響を通じて損失に影響を与え、線形に影響します。 
δ
私
{\displaystyle \delta ^{l}} は、レイヤーでのウェイトの勾配を計算するために必要な唯一のデータです 
私
{\displaystyle l} とすると、前の層を計算できます 
δ
私
−
1
{\displaystyle \delta ^{l-1}} と再帰的に繰り返されます。これにより、2 つの方法で非効率が回避されます。まず、レイヤーでの勾配を計算するときに重複を避けることができます。 
私
{\displaystyle l}、後の層ですべての導関数を再計算する必要はありません 
私
+
1
、
私
+
2
、
…{\displaystyle l+1,l+2,\ldots } 毎回。第 2 に、重みの変化に関する隠れ層の値の導関数を不必要に計算するのではなく、各段階で最終的な出力 (損失) に対する重みの勾配を直接計算するため、不必要な中間計算が回避されます。 
∂
ある
j
』
私
』
/
∂
w
j
k
私
{\displaystyle \partial a_{j'}^{l'}/\partial w_{jk}^{l}}。

バックプロパゲーションは、単純なフィードフォワード ネットワークに対して行列乗算の観点から、またはより一般的には随伴グラフの観点から表現できます。

行列乗算
フィードフォワード ネットワークの基本的なケースでは、各層のノードが (層をスキップせずに) すぐ次の層のノードにのみ接続され、最終出力のスカラー損失を計算する損失関数があり、バックプロパゲーションは単純に行列の乗算によって理解できます。[c] 本質的に、バックプロパゲーションは、コスト関数の導関数の式を右から左への各層間の導関数の積として、右から左へ (「逆方向に」) 評価します。各層間の重みの勾配は部分積の単純な変更です (「逆伝播誤差」)。

入力と出力のペアが与えられた場合 
(
×
、
y
)
{\displaystyle (x,y)}、損失は次のとおりです。

C
(
y
、
f
L
(
W
L
f
L
−
1
(
W
L
−
1
⋯
f
2
(
W
2
f
1
(
W
1
×
)
)
⋯
)
)
)
{\displaystyle C(y,f^{L}(W^{L}f^{L-1}(W^{L-1}\cdots f^{2}(W^{2}f^{1}(W^{1}x))\cdots )))}
これを計算するには、入力から開始します。 
×
{\displaystyle x} と前進します。各隠れ層の重み付けされた入力を次のように示します。 
z
私
{\displaystyle z^{l}} と隠れ層の出力 
私
アクティベーションとして {\displaystyle l} 
ある
私
{\displaystyle a^{l}}。バックプロパゲーションの場合、活性化 
ある
私
{\displaystyle a^{l}} とその派生関数 
(
f
私
)
』
{\displaystyle (f^{l})'} (評価時 
z
私
{\displaystyle z^{l}}) は、バックワード パス中に使用できるようにキャッシュする必要があります。

入力に関する損失の導関数は連鎖則によって与えられます。各項は合計導関数であり、入力のネットワーク (各ノード) の値で評価されることに注意してください。 
×
{\displaystyle x}:

d
C
d
ある
L
⋅
d
ある
L
d
z
L
⋅
d
z
L
d
ある
L
−
1
⋅
d
ある
L
−
1
d
z
L
−
1
⋅
d
z
L
−
1
d
ある
L
−
2
⋅
…
⋅
d
ある
1
d
z
1
⋅
∂
z
1
∂
×
、
{\displaystyle {\frac {dC}{da^{L}}}\cdot {\frac {da^{L}}{dz^{L}}}\cdot {\frac {dz^{L}}{da^{L-1}}}\cdot {\frac {da^{L-1}}{dz^{L-1}}}\cdot {\frac {dz^{L-1}}{da^{L-2}}}\cdot \ldots \cdot {\frac {da^{1}}{dz^{1}}}\cdot {\frac {\partial z^{1}}{\partial x}},}
どこで 
d
ある
L
d
z
L
{\displaystyle {\frac {da^{L}}{dz^{L}}}} は対角行列です。

これらの項は次のとおりです: 損失関数の導関数、[d] 活性化関数の導関数、[e]、および重みの行列: [f]

d
C
d
ある
L
○
(
f
L
)
』
⋅
W
L
○
(
f
L
−
1
)
』
⋅
W
L
−
1
○
⋯
○
(
f
1
)
』
⋅
W
1
。
{\displaystyle {\frac {dC}{da^{L}}}\circ (f^{L})'\cdot W^{L}\circ (f^{L-1})'\cdot W^{L-1}\circ \cdots \circ (f^{1})'\cdot W^{1}.}
グラデーション 
∇{\displaystyle \nabla } は入力に関する出力の微分の転置であるため、行列は転置され、乗算の順序は逆になりますが、エントリは同じです。

∇
×
C
=
(
W
1
)
T
⋅
(
f
1
)
』
○
…
○
(
W
L
−
1
)
T
⋅
(
f
L
−
1
)
』
○
(
W
L
)
T
⋅
(
f
L
)
』
○
∇
ある
L
C
。
{\displaystyle \nabla _{x}C=(W^{1})^{T}\cdot (f^{1})'\circ \ldots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}
バックプロパゲーションは基本的に、この式を右から左に評価し (導関数に対する前の式を左から右に乗算することと同等)、途中の各層での勾配を計算することで構成されます。重みの勾配は単なる部分式ではないため、追加のステップがあり、追加の乗算が行われます。

補助量の紹介 
δ
私
部分積 (右から左への乗算) の {\displaystyle \delta ^{l}}、「レベルでのエラー」として解釈されます。 
私
{\displaystyle l}" であり、レベルでの入力値の勾配として定義されます。 
私
{\displaystyle l}:

δ
私
:=
(
f
私
)
』
○
(
W
私
+
1
)
T
⋅
(
f
私
+
1
)
』
○
⋯
○
(
W
L
−
1
)
T
⋅
(
f
L
−
1
)
』
○
(
W
L
)
T
⋅
(
f
L
)
』
○
∇
ある
L
C
。
{\displaystyle \delta ^{l}:=(f^{l})'\circ (W^{l+1})^{T}\cdot (f^{l+1})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C.}
注意してください 
δ
私
{\displaystyle \delta ^{l}} は、レベル内のノードの数に等しい長さのベクトルです。 
私
{\displaystyle l};各コンポーネントは「そのノード（の値）に起因するコスト」として解釈されます。

レイヤー内のウェイトの勾配 
私
{\displaystyle l} は次のようになります。

∇
W
私
C
=
δ
私
(
ある
私
−
1
)
T
。
{\displaystyle \nabla _{W^{l}}C=\delta ^{l}(a^{l-1})^{T}.}
の要因 
ある
私
−
1
{\displaystyle a^{l-1}} は重みが 
W
私
レベル間の {\displaystyle W^{l}} 
私
−
1
{\displaystyle l-1} と 
私
{\displaystyle l} 影響レベル 
私
{\displaystyle l} は入力 (アクティベーション) に比例します。入力は固定されており、重みは変化します。

の 
δ
私
{\displaystyle \delta ^{l}} は、次のように右から左に再帰的に簡単に計算できます。

δ
私
−
1
:=
(
f
私
−
1
)
』
○
(
W
私
)
T
⋅
δ
私
。
{\displaystyle \delta ^{l-1}:=(f^{l-1})'\circ (W^{l})^{T}\cdot \delta ^{l}.}
したがって、重みの勾配は、レベルごとにいくつかの行列の乗算を使用して計算できます。これがバックプロパゲーションです。

単純な前方計算 ( 
δ
私
{\displaystyle \delta ^{l}} 例):

δ
1
=
(
f
1
)
』
○
(
W
2
)
T
⋅
(
f
2
)
』
○
⋯
○
(
W
L
−
1
)
T
⋅
(
f
L
−
1
)
』
○
(
W
L
)
T
⋅
(
f
L
)
』
○
∇
ある
L
C
δ
2
=
(
f
2
)
』
○
⋯
○
(
W
L
−
1
)
T
⋅
(
f
L
−
1
)
』
○
(
W
L
)
T
⋅
(
f
L
)
』
○
∇
ある
L
C
⋮
δ
L
−
1
=
(
f
L
−
1
)
』
○
(
W
L
)
T
⋅
(
f
L
)
』
○
∇
ある
L
C
δ
L
=
(
f
L
)
』
○
∇
ある
L
C
、
{\displaystyle {\begin{aligned}\delta ^{1}&=(f^{1})'\circ (W^{2})^{T}\cdot (f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{2}&=(f^{2})'\circ \cdots \circ (W^{L-1})^{T}\cdot (f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\&\vdots \\\delta ^{L-1}&=(f^{L-1})'\circ (W^{L})^{T}\cdot (f^{L})'\circ \nabla _{a^{L}}C\\\delta ^{L}&=(f^{L})'\circ \nabla _{a^{L}}C,\end{aligned}}}
バックプロパゲーションには 2 つの重要な違いがあります。

コンピューティング 
δ
私
−
1
{\displaystyle \delta ^{l-1}} に関して 
δ
私
{\displaystyle \delta ^{l}} は、レイヤーの明らかな重複した乗算を回避します。 
私
{\displaystyle l} 以降。
から始まる乗算 
∇
ある
L
C
{\displaystyle \nabla _{a^{L}}C} – 誤差を逆方向に伝播する – は、各ステップが単にベクトルを乗算することを意味します (
δ
私
{\displaystyle \delta ^{l}}) 重みの行列による 
(
W
私
)
T
{\displaystyle (W^{l})^{T}} とアクティベーションの派生関数 
(
f
私
−
1
)
』
{\displaystyle (f^{l-1})'}。対照的に、前の層での変更から開始して順方向に乗算することは、各乗算で行列と行列を乗算することを意味します。これははるかにコストが高く、1 つのレイヤーで考えられる変更のすべてのパスを追跡することに相当します。 
私
{\displaystyle l} レイヤー内の変更に進みます 
私
+
2
{\displaystyle l+2} (乗算用) 
W
私
+
1
{\displaystyle W^{l+1}} 作成者 
W
私
+
2
{\displaystyle W^{l+2}}、アクティベーションの導関数に対する追加の乗算を伴う)、重みの変更が隠れノードの値にどのような影響を与えるかについての中間量を不必要に計算します。
随伴グラフ
[アイコン]	
このセクションは拡張が必要です。追加していただくと助かります。 （2019年11月）
より一般的なグラフやその他の高度なバリエーションでは、バックプロパゲーションは自動微分という観点から理解できます。バックプロパゲーションは逆累積 (または「逆モード」) の特殊なケースです。[5]

直感
モチベーション
教師あり学習アルゴリズムの目標は、一連の入力を正しい出力に最適にマッピングする関数を見つけることです。バックプロパゲーションの動機は、入力から出力への任意のマッピングを学習できるように、適切な内部表現を学習できるように多層ニューラル ネットワークをトレーニングすることです。[8]

最適化問題としての学習
バックプロパゲーション アルゴリズムの数学的導出を理解するには、まずニューロンの実際の出力と特定のトレーニング サンプルの正しい出力との関係について直観を養うことが役立ちます。 2 つの入力ユニット、1 つの出力ユニットを備え、隠れユニットはなく、各ニューロンが線形出力 (入力から出力へのマッピングが非線形であるニューラル ネットワークに関するほとんどの研究とは異なります) [g] を使用する単純なニューラル ネットワークを考えてみましょう。これは入力の重み付き合計です。


2 つの入力ユニット (それぞれ 1 つの入力を持つ) と 1 つの出力ユニット (2 つの入力を持つ) を備えた単純なニューラル ネットワーク
最初は、トレーニングの前に、重みがランダムに設定されます。次に、ニューロンはトレーニング例から学習します。この場合、トレーニング例は一連のタプルで構成されます。 
(
×
1
、
×
2
、
t
)
{\displaystyle (x_{1},x_{2},t)} ここで 
×
1
{\displaystyle x_{1}} と 
×
2
{\displaystyle x_{2}} はネットワークへの入力であり、 t は正しい出力 (トレーニング時にネットワークがこれらの入力を与えられて生成する出力) です。与えられた初期ネットワーク 
×
1
{\displaystyle x_{1}} と 
×
2
{\displaystyle x_{2}} は、t とは異なる可能性が高い出力 y を計算します (ランダムな重みが与えられる)。損失関数 
L
(
t
、
y
)
{\displaystyle L(t,y)} は、ターゲット出力 t と計算出力 y の間の不一致を測定するために使用されます。回帰分析問題の場合は二乗誤差を損失関数として使用でき、分類にはカテゴリクロスエントロピーを使用できます。

例として、二乗誤差を損失として使用する回帰問題を考えてみましょう。

L
(
t
、
y
)
=
(
t
−
y
)
2
=
E
、
{\displaystyle L(t,y)=(t-y)^{2}=E,}
ここで、E は不一致またはエラーです。

単一のトレーニング ケースでネットワークを考えてみましょう。 
(
1
、
1
、
0
)
{\displaystyle (1,1,0)}。したがって、入力は 
×
1
{\displaystyle x_{1}} と 
×
2
{\displaystyle x_{2}} はそれぞれ 1 と 1 で、正しい出力 t は 0 です。ここで、ネットワークの出力 y を横軸に、誤差 E を縦軸にプロットすると、結果は放物線になります。放物線の最小値は、誤差 E を最小化する出力 y に対応します。単一のトレーニング ケースでは、最小値も水平軸に触れます。これは、誤差がゼロになり、ネットワークがターゲット出力 t に正確に一致する出力 y を生成できることを意味します。したがって、入力を出力にマッピングする問題は、最小の誤差を生成する関数を見つけるという最適化問題に還元できます。


単一トレーニング ケースの線形ニューロンの誤差曲面
ただし、ニューロンの出力は、すべての入力の加重合計に依存します。

y
=
×
1
w
1
+
×
2
w
2
、
{\displaystyle y=x_{1}w_{1}+x_{2}w_{2},}
どこで 
w
1
{\displaystyle w_{1}} と 
w
2
{\displaystyle w_{2}} は、入力ユニットから出力ユニットへの接続の重みです。したがって、誤差はニューロンに入力される重みにも依存します。これは、学習を可能にするために最終的にネットワーク内で変更する必要があるものです。

この例では、トレーニング データを挿入するときに、 
(
1
、
1
、
0
)
{\displaystyle (1,1,0)}、損失関数は次のようになります。

E
=
(
t
−
y
)
2
=
y
2
=
(
×
1
w
1
+
×
2
w
2
)
2
=
(
w
1
+
w
2
)
2
。
{\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}

次に、損失関数は、 
E
{\displaystyle E} は、底面が平行に向いた放物線状の円柱の形をとります。 
w
1
=
−
w
2
{\displaystyle w_{1}=-w_{2}}。を満たす重みのセットはすべてあるので、 
w
1
=
−
w
2
{\displaystyle w_{1}=-w_{2}} は損失関数を最小化します。この場合、一意の解に収束するには追加の制約が必要です。追加の制約は、重みに特定の条件を設定するか、追加のトレーニング データを注入することによって生成できます。

誤差を最小限に抑える重みのセットを見つけるためによく使用されるアルゴリズムの 1 つは勾配降下法です。バックプロパゲーションにより、現在のシナプス重みに対する損失関数の最も急な降下方向が計算されます。その後、最も急な降下方向に沿って重みを変更することができ、効率的な方法で誤差を最小限に抑えることができます。

導出
勾配降下法では、ネットワークの重みに関する損失関数の導関数を計算します。これは通常、バックプロパゲーションを使用して行われます。出力ニューロンが 1 つあると仮定すると、[h] 二乗誤差関数は次のようになります。

E
=
L
(
t
、
y
)
{\displaystyle E=L(t,y)}
どこで

L
{\displaystyle L} は出力の損失です 
y
{\displaystyle y} と目標値 
t
{\displaystyle t}、
t
{\displaystyle t} はトレーニング サンプルのターゲット出力であり、
y
{\displaystyle y} は、出力ニューロンの実際の出力です。
ニューロンごとに 
j
{\displaystyle j}、その出力 
ああ
j
{\displaystyle o_{j}} は次のように定義されます

ああ
j
=
φ
(
ネット
j
)
=
φ
(
∑
k
=
1
n
w
k
j
×
k
)
、
{\displaystyle o_{j}=\varphi ({\text{net}}_{j})=\varphi \left(\sum _{k=1}^{n}w_{kj}x_{k}\right),}
ここで活性化関数は 
φ{\displaystyle \varphi } は非線形であり、活性化領域にわたって微分可能です (ReLU はある点では微分可能ではありません)。歴史的に使用されてきた活性化関数はロジスティック関数です。

φ
(
z
)
=
1
1
+
e
−
z
{\displaystyle \varphi (z)={\frac {1}{1+e^{-z}}}}
これには次のような便利な導関数があります。

d
φ
d
z
=
φ
(
z
)
(
1
−
φ
(
z
)
)
{\displaystyle {\frac {d\varphi }{dz}}=\varphi (z)(1-\varphi (z))}
入力 
ネット
j
ニューロンへの {\displaystyle {\text{net}}_{j}} は出力の加重合計です 
ああ
k
以前のニューロンの {\displaystyle o_{k}}。ニューロンが入力層の後の最初の層にある場合、 
ああ
k
入力層の {\displaystyle o_{k}} は単なる入力です 
×
k
{\displaystyle x_{k}} をネットワークに接続します。ニューロンへの入力ユニットの数は 
n
{\displaystyle n}。変数 
w
k
j
{\displaystyle w_{kj}} はニューロン間の重みを表します 
k
前の層とニューロンの {\displaystyle k} 
j
現在のレイヤーの {\displaystyle j}。

誤差の導関数を求める

ここで使用される表記法を説明するための人工ニューラル ネットワークの図
重みに関する誤差の偏導関数を計算する 
w
私は
j
{\displaystyle w_{ij}} はチェーン ルールを 2 回使用して実行されます。

∂
E
∂
w
私は
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
w
私は
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
ネット
j
∂
ネット
j
∂
w
私は
j
{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}} (式 1)
上記の右辺の最後の因数では、合計に 1 つの項のみが含まれます。 
ネット
j
{\displaystyle {\text{net}}_{j}} は以下に依存します 
w
私は
j
{\displaystyle w_{ij}}、つまり

∂
ネット
j
∂
w
私は
j
=
∂
∂
w
私は
j
(
∑
k
=
1
n
w
k
j
ああ
k
)
=
∂
∂
w
私は
j
w
私は
j
ああ
私は
=
ああ
私は
。
{\displaystyle {\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial }{\partial w_{ij}}}\left(\sum _{k=1}^{n}w_{kj}o_{k}\right)={\frac {\partial }{\partial w_{ij}}}w_{ij}o_{i}=o_{i}.} (式 2)
ニューロンが入力層の後の最初の層にある場合、 
ああ
私は
{\displaystyle o_{i}} は単なる 
×
私は
{\displaystyle x_{i}}。

ニューロンの出力の導関数 
j
入力に関する {\displaystyle j} は、単に活性化関数の偏導関数です。

∂
ああ
j
∂
ネット
j
=
∂
φ
(
ネット
j
)
∂
ネット
j
{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial \varphi ({\text{net}}_{j})}{\partial {\text{net}}_{j}}}} (式 3)
ロジスティック活性化関数の場合

∂
ああ
j
∂
ネット
j
=
∂
∂
ネット
j
φ
(
ネット
j
)
=
φ
(
ネット
j
)
(
1
−
φ
(
ネット
j
)
)
=
ああ
j
(
1
−
ああ
j
)
{\displaystyle {\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\frac {\partial }{\partial {\text{net}}_{j}}}\varphi ({\text{net}}_{j})=\varphi ({\text{net}}_{j})(1-\varphi ({\text{net}}_{j}))=o_{j}(1-o_{j})}
これが、バックプロパゲーションが活性化関数が微分可能であることを必要とする理由です。 (それでも、0 で微分不可能な ReLU 活性化関数は、AlexNet などで非常に人気があります)

最初の要素は、ニューロンが出力層にあるかどうかを評価するのが簡単です。 
ああ
j
=
y
{\displaystyle o_{j}=y} および

∂
E
∂
ああ
j
=
∂
E
∂
y
{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}} (式 4)
二乗誤差の半分が損失関数として使用される場合、次のように書き換えることができます。

∂
E
∂
ああ
j
=
∂
E
∂
y
=
∂
∂
y
1
2
(
t
−
y
)
2
=
y
−
t
{\displaystyle {\frac {\partial E}{\partial o_{j}}}={\frac {\partial E}{\partial y}}={\frac {\partial }{\partial y}}{\frac {1}{2}}(t-y)^{2}=y-t}
ただし、 
j
{\displaystyle j} はネットワークの任意の内部層にあり、導関数を求めます。 
E
{\displaystyle E} に関して 
ああ
j
{\displaystyle o_{j}} はそれほど明確ではありません。

検討中 
E
すべてのニューロンを入力とする関数としての {\displaystyle E} 
L
=
{
あなた
、
v
、
…
、
w
}
{\displaystyle L=\{u,v,\dots ,w\}} ニューロンから入力を受け取る 
j
{\displaystyle j}、

∂
E
(
ああ
j
)
∂
ああ
j
=
∂
E
(
n
e
t
あなた
、
ネット
v
、
…
、
n
e
t
w
)
∂
ああ
j
{\displaystyle {\frac {\partial E(o_{j})}{\partial o_{j}}}={\frac {\partial E(\mathrm {net} _{u},{\text{net}}_{v},\dots ,\mathrm {net} _{w})}{\partial o_{j}}}}
についての合計微分値を計算します。 
ああ
j
{\displaystyle o_{j}}、導関数の再帰式が得られます。

∂
E
∂
ああ
j
=
∑
ℓ
∈
L
(
∂
E
∂
ネット
ℓ
∂
ネット
ℓ
∂
ああ
j
)
=
∑
ℓ
∈
L
(
∂
E
∂
ああ
ℓ
∂
ああ
ℓ
∂
ネット
ℓ
∂
ネット
ℓ
∂
ああ
j
)
=
∑
ℓ
∈
L
(
∂
E
∂
ああ
ℓ
∂
ああ
ℓ
∂
ネット
ℓ
w
j
ℓ
)
{\displaystyle {\frac {\partial E}{\partial o_{j}}}=\sum _{\ell \in L}\left({\frac {\partial E}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}{\frac {\partial {\text{net}}_{\ell }}{\partial o_{j}}}\right)=\sum _{\ell \in L}\left({\frac {\partial E}{\partial o_{\ell }}}{\frac {\partial o_{\ell }}{\partial {\text{net}}_{\ell }}}w_{j\ell }\right)} (式 5)
したがって、次の導関数は、 
ああ
j
{\displaystyle o_{j}} は、出力に関するすべての導関数を計算できます。 
ああ
次の層 (出力ニューロンに近い層) の ℓ{\displaystyle o_{\ell }} は既知です。 [セット内のニューロンのいずれかが 
L
{\displaystyle L} はニューロンに接続されていませんでした 
j
{\displaystyle j} の場合、それらは独立しています。 
w
私は
j
{\displaystyle w_{ij}} と合計の対応する偏導関数は 0 に消えます。]

式を代入すると、 2、式3 式 4 および式 4式の 5 1 得られるもの:

∂
E
∂
w
私は
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
ネット
j
∂
ネット
j
∂
w
私は
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
ネット
j
ああ
私は
{\displaystyle {\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}o_{i}}
∂
E
∂
w
私は
j
=
ああ
私は
δ
j
{\displaystyle {\frac {\partial E}{\partial w_{ij}}}=o_{i}\delta _{j}}
と

δ
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
ネット
j
=
{
∂
L
(
t
、
ああ
j
)
∂
ああ
j
d
φ
(
ネット
j
)
d
ネット
j
もし 
j
 は出力ニューロンであり、
(
∑
ℓ
∈
L
w
j
ℓ
δ
ℓ
)
d
φ
(
ネット
j
)
d
ネット
j
もし 
j
 内部ニューロンです。
{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}{\frac {\partial L(t,o_{j})}{\partial o_{j}}}{\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&{\text{if }}j{\text{ は出力ニューロンです。}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell }){\frac {d\varphi ({\text{net}}_{j})}{d{\text{net}}_{j}}}&{\text{if }}j{\text{ は内部ニューロンです。}}\end{cases}}}
もし 
φ{\displaystyle \varphi } はロジスティック関数であり、誤差は二乗誤差です。

δ
j
=
∂
E
∂
ああ
j
∂
ああ
j
∂
ネット
j
=
{
(
ああ
j
−
t
j
)
ああ
j
(
1
−
ああ
j
)
もし 
j
 は出力ニューロンであり、
(
∑
ℓ
∈
L
w
j
ℓ
δ
ℓ
)
ああ
j
(
1
−
ああ
j
)
もし 
j
 内部ニューロンです。
{\displaystyle \delta _{j}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}={\begin{cases}(o_{j}-t_{j})o_{j}(1-o_{j})&{\text{if }}j{\text{ は出力ですニューロン、}}\\(\sum _{\ell \in L}w_{j\ell }\delta _{\ell })o_{j}(1-o_{j})&{\text{if }}j{\text{ は内部ニューロンです。}}\end{cases}}}
体重を更新するには 
w
私は
j
{\displaystyle w_{ij}} 勾配降下法を使用する場合、学習率を選択する必要があります。 
η
>
0
{\displaystyle \eta >0}。体重の変化は、次のような影響を反映する必要があります。 
E
{\displaystyle E} の増加または減少 
w
私は
j
{\displaystyle w_{ij}}。もし 
∂
E
∂
w
私は
j
>
0
{\displaystyle {\frac {\partial E}{\partial w_{ij}}}>0}、増加 
w
私は
j
{\displaystyle w_{ij}} が増加します 
E
{\displaystyle E};逆に、もし 
∂
E
∂
w
私は
j
<
0
{\displaystyle {\frac {\partial E}{\partial w_{ij}}}<0}、増加 
w
私は
j
{\displaystyle w_{ij}} が減少します 
E
{\displaystyle E}。新しい 
Δ
w
私は
j
{\displaystyle \Delta w_{ij}} が古い重みに追加され、学習率と勾配の積に次の値が乗算されます。 
−
1
{\displaystyle -1} は次のことを保証します 
w
私は
j
{\displaystyle w_{ij}} は常に減少するように変化します 
E
{\displaystyle E}。つまり、すぐ下の式では、 
−
η
∂
E
∂
w
私は
j
{\displaystyle -\eta {\frac {\partial E}{\partial w_{ij}}}} は常に変更されます 
w
私は
j
{\displaystyle w_{ij}} を次のような方法で実行します。 
E
{\displaystyle E} が減少します:

Δ
w
私は
j
=
−
η
∂
E
∂
w
私は
j
=
−
η
ああ
私は
δ
j
{\displaystyle \Delta w_{ij}=-\eta {\frac {\partial E}{\partial w_{ij}}}=-\eta o_{i}\delta _{j}}
二次勾配降下法
誤差関数の 2 次導関数のヘッセ行列を使用するレーベンバーグ・マルカート アルゴリズムは、特に誤差関数のトポロジーが複雑な場合、多くの場合、1 次勾配降下法よりも速く収束します。[9][10]また、他の方法では収束しない可能性のある、より少ないノード数での解を見つけることもできます。[10]ヘッセ行列は、フィッシャー情報行列によって近似できます。[11]

損失関数
詳細情報: 損失関数
損失関数は、1 つまたは複数の変数の値を、それらの値に関連付けられた「コスト」を直観的に表す実数にマッピングする関数です。バックプロパゲーションの場合、損失関数は、トレーニング サンプルがネットワークを介して伝播した後、ネットワーク出力とその期待される出力の差を計算します。

仮定
損失関数の数式がバックプロパゲーションで使用されるためには、2 つの条件を満たす必要があります [12]。 1 つ目は、平均として書くことができるということです。 
E
=
1
n
∑
×
E
×
エラー関数に対する {\textstyle E={\frac {1}{n}}\sum _{x}E_{x}} 
E
×
{\textstyle E_{x}}、用 
n
{\textstyle n} 個のトレーニング例、 
×
{\textstyle x}。この仮定の理由は、バックプロパゲーション アルゴリズムが単一のトレーニング サンプルの誤差関数の勾配を計算するためであり、これを誤差関数全体に一般化する必要があるためです。 2 番目の仮定は、ニューラル ネットワークからの出力の関数として記述できるということです。

損失関数の例
しましょう 
y
、
y
』
{\displaystyle y,y'} はベクトルになります 
R
n
{\displaystyle \mathbb {R} ^{n}}。

誤差関数を選択してください 
E
(
y
、
y
』
)
{\displaystyle E(y,y')} 2 つの出力の差を測定します。標準的な選択は、ベクトル間のユークリッド距離の 2 乗です。 
y
{\displaystyle y} と 
y
』
{\displaystyle y'}:
E
(
y
、
y
』
)
=
1
2
‖
y
−
y
』
‖
2
{\displaystyle E(y,y')={\tfrac {1}{2}}\lVert y-y'\rVert ^{2}}エラー関数が終了しました 
n
{\textstyle n} トレーニング サンプルは、個々のサンプルの損失の平均として記述することができます。
E
=
1
2
n
∑
×
‖
(
y
(
×
)
−
y
』
(
×
)
)
‖
2
{\displaystyle E={\frac {1}{2n}}\sum _{x}\lVert (y(x)-y'(x))\rVert ^{2}}

制限事項

勾配降下法では、グローバル最小値の代わりにローカル最小値が見つかる場合があります。
バックプロパゲーションを使用した勾配降下法では、誤差関数の大域的最小値が見つかることは保証されておらず、局所的最小値が見つかるだけです。また、誤差関数の状況においてプラトーを越えるのが困難です。この問題は、ニューラル ネットワークの誤差関数の非凸性によって引き起こされ、長い間大きな欠点であると考えられていましたが、Yann LeCun et al.多くの現実的な問題ではそうではない、と主張している[13]。
バックプロパゲーション学習では、入力ベクトルの正規化は必要ありません。ただし、正規化によりパフォーマンスが向上する可能性があります。[14]
バックプロパゲーションでは、ネットワーク設計時に活性化関数の導関数がわかっている必要があります。
歴史
参照: パーセプトロンの歴史
前駆体
バックプロパゲーションは本質的にチェーン ルール (1676 年にゴットフリート ヴィルヘルム ライプニッツによって最初に書き留められた[15][16]) をニューラル ネットワークに効率的に適用するものであるため、繰り返し導出されてきました。

「逆伝播誤り訂正」という用語は 1962 年に Frank Rosenblatt によって導入されましたが、彼はこれを実装する方法を知りませんでした [17]。いずれにせよ、彼は出力が離散レベルであり、導関数がゼロであるため逆伝播が不可能であるニューロンのみを研究しました。

バックプロパゲーションの前兆は、1950 年代以降の最適制御理論に登場しました。 Yann LeCun らは、最適制御理論、特に随伴状態法における Pontryagin らによる 1950 年代の研究がバックプロパゲーションの連続時間バージョンであると認めています [18]。 Hecht-Nielsen [19] は、Robbins-Monro アルゴリズム (1951 年) [20] と Arthur Bryson と Yu-Chi Ho の Applied Optimal Control (1969 年) をバックプロパゲーションの予兆として認めています。他の先駆者は、Henry J. Kelley 1960 [1] と Arthur E. Bryson (1961) [2] です。 1962 年に、Stuart Dreyfus は連鎖則のみに基づいたより単純な導出を発表しました。[21][22][23] 1973 年に、彼は誤差勾配に比例してコントローラーのパラメーターを適応させました。[24]現代のバックプロパゲーションとは異なり、これらのプリカーサーでは、あるステージから前のステージまで標準的なヤコビ行列計算が使用され、複数のステージにわたる直接リンクや、ネットワークの疎性による潜在的な追加の効率向上には対応していませんでした。

ADALINE (1960) 学習アルゴリズムは、単一層の二乗誤差損失を伴う勾配降下法でした。確率的勾配降下法によって訓練された複数の層を備えた最初の多層パーセプトロン (MLP) [20] は、1967 年に甘利俊一によって公開されました [26]。 MLP には 5 つの層があり、そのうち 2 つの学習可能な層があり、線形分離できないパターンを分類することを学習しました。

最新のバックプロパゲーション
現代の逆伝播は、ネストされた微分可能関数の離散接続ネットワーク用の「自動微分の逆モード」(1970)[27]として Seppo Linnainmaa によって最初に発表されました。[28][29][30]

1982 年、Paul Werbos は、標準となっている方法で MLP にバックプロパゲーションを適用しました。[31][32]ウェルボス氏はインタビューでバックプロパゲーションをどのように開発したかについて説明しました。 1971 年、博士課程の研究中に、フロイトの「精神的エネルギーの流れ」を数学化する逆伝播法を開発しました。彼は作品の出版において何度も困難に直面し、1981年にようやく出版を終えることができた[33]。

1982 年頃、[33]: 376 David E. Rumelhart は独自にバックプロパゲーション [34]: 252 を開発し、そのアルゴリズムを研究サークルの他の人々に教えました。彼は以前の研究を知らなかったので引用しなかった。彼は最初にアルゴリズムを 1985 年の論文で発表し、次に 1986 年の Nature 論文でこのテクニックの実験的分析を発表しました。これらの論文は高く引用されるようになり、バックプロパゲーションの普及に貢献し、1980 年代にニューラル ネットワークに対する研究への関心が再び高まったのと一致しました。[8][36][37]。

1985 年に、この方法は David Parker によっても説明されました [38] [39]。 Yann LeCun は 1987 年の博士論文で、ニューラル ネットワークの逆伝播の代替形式を提案しました。[40]

勾配降下法は受け入れられるまでにかなりの時間がかかりました。初期の反対意見には次のようなものがありました。勾配降下法が大域最小値に到達できるという保証はなく、局所最小値にのみ到達できるという保証はありません。ニューロンは、連続信号ではなく離散信号 (0/1) を生成するものとして生理学者によって「知られ」ており、離散信号の場合は勾配を取る必要がありません。ジェフリー・ヒントンのインタビューをご覧ください。[33]

初期の成功
バックプロパゲーションを介したニューラル ネットワークのトレーニングにおけるいくつかのアプリケーションが受け入れに貢献し、時には研究サークルの外で人気を博しました。

1987 年、NETtalk は英語のテキストを発音に変換することを学習しました。 Sejnowski はバックプロパゲーションとボルツマン マシンの両方でトレーニングを試みましたが、バックプロパゲーションの方が大幅に高速であることがわかったので、それを最終的な NETtalk に使用しました。[33]: 324 NETtalk プログラムは人気のある成功となり、Today ショーに出演しました。[41]

1989 年、学部長 A. ポメルローは、バックプロパゲーションを使用して自律的に駆動するように訓練されたニューラル ネットワークである ALVINN を発表しました。

LeNet は、手書きの郵便番号を認識するために 1989 年に発行されました。

1992 年、TD ギャモンはバックギャモンで人間のトップレベルのプレイを達成しました。これは、逆伝播によって訓練された 2 層のニューラル ネットワークを備えた強化学習エージェントでした。[43]

1993 年、エリック ワンはバックプロパゲーションによる国際パターン認識コンテストで優勝しました [44] [45]。

バックプロパゲーション後
2000 年代には人気が落ちましたが [要出典] 、安価で強力な GPU ベースのコンピューティング システムの恩恵を受けて 2010 年代に復活しました。これは、音声認識、マシンビジョン、自然言語処理、および言語構造学習の研究において特に当てはまります（第一言語学習[46]および第二言語学習に関連するさまざまな現象を説明するために言語構造が使用されてきました[48]）。

誤差逆伝播は、N400 や P600 などの人間の脳の事象関連電位 (ERP) コンポーネントを説明するために提案されています [49]。

2023 年に、スタンフォード大学のチームによってバックプロパゲーション アルゴリズムがフォトニック プロセッサに実装されました [50]。

