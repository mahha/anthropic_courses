大規模言語モデル (LLM) は、汎用言語生成や分類などのその他の自然言語処理タスクを実現できることで知られる計算モデルです。 LLM は、言語モデルに基づいて、計算負荷の高い自己教師ありおよび半教師ありのトレーニング プロセス中に膨大な量のテキストから統計的関係を学習することで、これらの能力を獲得します。[1] LLM は、入力テキストを取得し、次のトークンまたは単語を繰り返し予測することにより、生成 AI の一種であるテキスト生成に使用できます。[2]

LLM は、2017 年に発明されたトランスフォーマー アーキテクチャを使用する人工ニューラル ネットワークです。2024 年 6 月の時点で、最大かつ最も機能的な LLM は、デコーダーのみのトランスフォーマー ベースのアーキテクチャで構築されており、大規模なテキスト データの効率的な処理と生成を可能にします。

歴史的には、2020 年までは微調整がモデルを特定のタスクに適応させるために使用される主な方法でした。ただし、GPT-3 などのより大きなモデルでは、モデルの応答をガイドする特定の入力プロンプトを作成するプロンプト エンジニアリングを通じて同様の結果を達成できることが実証されています [3]。これらのモデルは、人間の言語コーパスに固有の構文、意味論、オントロジーに関する知識を獲得します[4]が、トレーニングの対象となるデータに存在する不正確さとバイアスも継承します[5]。

注目すべき LLM には、OpenAI の GPT シリーズ モデル (例: GPT-3.5、GPT-4、GPT-4o、ChatGPT および Microsoft Copilot で使用)、Google の Gemini (後者は現在同じ名前のチャットボットで使用されている)、Meta の LLaMA ファミリーのモデル、Watsonx とともに最初にリリースされた IBM の Granite モデル、Anthropic の Claude モデル、Mistral などがあります。 AIのモデル。

歴史
2017 年以前は、当時利用可能な容量と比較して大規模な言語モデルがいくつかありました。 1990 年代に、IBM アラインメント モデルは統計言語モデリングの先駆けとなりました。 2001 年に 3 億語でトレーニングされた平滑化された n-gram モデルは、当時の SOTA の複雑さを達成しました。[6] 2000 年代にインターネットの使用が普及すると、一部の研究者はインターネット規模の言語データセット (「コーパスとしてのウェブ」[7]) を構築し、それに基づいて統計的言語モデルをトレーニングしました。[8][9] 2009 年には、大規模なデータセットを有効に取り込めるため、ほとんどの言語処理タスクにおいて、統計言語モデルが記号言語モデルよりも優勢でした。[10]

2012 年頃にニューラル ネットワークが画像処理で主流になった後、言語モデリングにもニューラル ネットワークが適用されました。 Google は 2016 年に翻訳サービスをニューラル機械翻訳に転換しました。Transformers 以前と同様に、seq2seq ディープ LSTM ネットワークによって行われていました。


元の論文の変圧器モデルの主要コンポーネントの図。マルチヘッド アテンションの後 (前ではなく) に層が正規化されています。
2017 年の NeurIPS カンファレンスで、Google の研究者は画期的な論文「Attending Is All You Need」でトランス アーキテクチャを紹介しました。この論文の目標は、2014 年の Seq2seq テクノロジー [11] を改良することであり、主に Bahdanau らによって開発されたアテンション メカニズムに基づいていました。 2014年に。[12]翌年の 2018 年に BERT が導入され、すぐに「ユビキタス」になりました [13]。元のトランスフォーマーにはエンコーダー ブロックとデコーダー ブロックの両方がありますが、BERT はエンコーダーのみのモデルです。

デコーダのみの GPT-1 は 2018 年に導入されましたが、広く注目を集めたのは 2019 年の GPT-2 でした。これは OpenAI が当初、悪意のある使用を恐れて一般公開するには強力すぎると判断したためです [14]。 2020 年の GPT-3 はさらに一歩進んで、2024 年時点では API 経由でのみ利用可能であり、モデルをダウンロードしてローカルで実行する機能は提供されていません。しかし、一般の人々の想像力を魅了し、メディアの誇大広告やオンラインの話題を引き起こしたのは、2022 年の消費者向けのブラウザベースの ChatGPT でした。[15] 2023 GPT-4 は精度の向上とマルチモーダル機能の「聖杯」として賞賛されました [16]。 OpenAIは、GPT-4の高レベルのアーキテクチャとパラメータの数を明らかにしなかった。

競合する言語モデルの大部分は、少なくともパラメーターの数に関して GPT シリーズと同等になろうとしています [17]。

2022 年以降、特に最初は BLOOM と LLaMA でソース利用可能なモデルの人気が高まっていますが、どちらも使用分野に制限があります。 Mistral AI のモデル Mistral 7B および Mixtral 8x7b には、より寛容な Apache ライセンスが付いています。 2024 年 6 月の時点で、Llama 3 の 700 億パラメーター モデルの命令で微調整されたバリアントは、LMSYS Chatbot Arena Leaderboard によると最も強力なオープン LLM であり、GPT-3.5 よりも強力ですが、GPT-4 ほど強力ではありません。[18]

2024 年の時点で、最大かつ最も機能的なモデルはすべて Transformer アーキテクチャに基づいています。最近の実装の一部は、リカレント ニューラル ネットワークのバリアントや Mamba (状態空間モデル) など、他のアーキテクチャに基づいています。[19][20][21]

データセットの前処理
参照: 機械学習研究用のデータセットのリスト § インターネット
トークン化
機械学習アルゴリズムはテキストではなく数値を処理するため、テキストを数値に変換する必要があります。最初のステップでは、語彙が決定され、次に整数インデックスが各語彙エントリに任意かつ一意に割り当てられ、最後に埋め込みが整数インデックスに関連付けられます。アルゴリズムには、バイト ペア エンコーディング (BPE) と WordPiece が含まれます。また、制御文字として機能する特別なトークンもあります。たとえば、マスクされたトークン (BERT で使用される) を表す [MASK] や、語彙に現れない文字を表す [UNK] (「不明」) などです。

たとえば、GPT-3 (レガシー) で使用される BPE トークナイザーは、トークナイザーをテキスト -> 一連の数値「トークン」として分割します。

トークナイザー: テキスト -> 一連の数値「tokens」
トークン化ではデータセットも圧縮されます。 LLM では通常、入力がギザギザではない配列である必要があるため、短いテキストは最長のテキストの長さに一致するまで「パディング」する必要があります。単語ごとに平均して必要なトークンの数は、データセットの言語によって異なります。[22][23]

BPE
詳細は「バイトペアエンコーディング」を参照
例として、バイトペアエンコーディングに基づくトークナイザーを考えてみましょう。最初のステップでは、すべての一意の文字 (空白と句読点を含む) が n グラムの初期セット (すなわち、ユニグラムの初期セット) として扱われます。続いて、最も頻繁に出現する隣接する文字のペアが 1 つのバイグラムにマージされ、そのペアのすべてのインスタンスがバイグラムに置き換えられます。最も頻繁に一緒に出現する (以前にマージされた) n グラムの隣接するペアのすべての出現は、規定のサイズの語彙が得られるまで、さらに長い n グラムに再びマージされます (GPT-3 の場合、サイズは 50257) [24]。トークナイザーがトレーニングされた後は、ユニグラムの初期セットに現れない文字が含まれていない限り、あらゆるテキストをトークナイザーによってトークン化できます。[25]

問題点
主に英語のコーパスから抽出された頻度に基づくトークン語彙では、平均的な英語の単語に対して可能な限り少ないトークンが使用されます。ただし、このような英語に最適化されたトークナイザーによってエンコードされた別の言語の平均的な単語は、最適ではない量のトークンに分割されます。 GPT-2 トークナイザーは、ミャンマーのシャン語など、一部の言語では単語ごとに最大 15 倍のトークンを使用できます。ポルトガル語やドイツ語など、さらに広く普及している言語では、英語と比較して「50% のプレミアム」が付いています。[26]

貪欲なトークン化も、テキスト補完に微妙な問題を引き起こします。[27]

データセットのクリーニング
詳細は「データクレンジング」を参照
LLM のトレーニングのコンテキストでは、データセットは通常、データセットから有害なパッセージを削除し、低品質のデータを破棄し、重複を排除することによってクリーンアップされます。[28]クリーンなデータセットはトレーニング効率を向上させ、ダウンストリームのパフォーマンスの向上につながります。[29][30]トレーニングされた LLM は、さらなる LLM をトレーニングするためのデータセットをクリーンアップするために使用できます。[31]

Web 上で LLM によって生成されたコンテンツの割合が増加しているため、将来のデータ クリーニングにはそのようなコンテンツのフィルタリングが含まれる可能性があります。 LLM で生成されたコンテンツが人間のテキストに似ている (フィルタリングが困難になる) ものの、品質が低い (そのコンテンツでトレーニングされたモデルのパフォーマンスが低下する) 場合、問題が発生する可能性があります。[32]

合成データ
詳細は「合成データ」を参照
最大の言語モデルのトレーニングには、自然に利用可能なデータよりも多くの言語データが必要になるか、自然に発生するデータの品質が不十分である可能性があります。このような場合、合成データが使用される可能性があります。 Microsoft の Phi シリーズの LLM は、別の LLM によって生成された教科書のようなデータに基づいてトレーニングされます。[33]

トレーニングとアーキテクチャ
参照: 微調整 (機械学習)
人間のフィードバックからの強化学習 (RLHF)
詳細は「人間のフィードバックからの強化学習」を参照
近接ポリシー最適化などのアルゴリズムによるヒューマン フィードバックからの強化学習 (RLHF) は、人間の好みのデータセットに基づいてモデルをさらに微調整するために使用されます。

命令チューニング
「自己指示」アプローチを使用することで、LLM は、いくつかのケースに対する人為的な修正から始めて、ナイーブな応答を置き換えて正しい応答をブートストラップすることができました。たとえば、「ハムレットで表現されている主要なテーマについてエッセイを書いてください」という指示では、コーパス内のこのテキストシーケンスの頻度に基づいて、最初の単純な完成は「3 月 17 日以降にエッセイを提出した場合、1 日遅れにつき成績が 10% 減点されます」となる可能性があります。[35]

専門家の混合
詳細は「専門家の混合」を参照
最大の LLM は、トレーニングして直接使用するには高価すぎる可能性があります。このようなモデルには、専門家混合 (MoE) を適用できます。これは、最大 1 兆パラメータに達するモデルをトレーニングするために 2017 年から Google 研究者によって追求されている一連の研究です。[36][37][38]。

プロンプトエンジニアリング、アテンションメカニズム、コンテキストウィンドウ
参照: プロンプト エンジニアリングと注意力 (機械学習)
以前は (コストのかかる) 微調整によってのみ達成できた結果のほとんどは、単一の会話の範囲に限定されますが (より正確にはコンテキスト ウィンドウの範囲に限定されます)、迅速なエンジニアリングによって達成できるようになります。[39]


各ヘッドが独自の基準に従って、他のトークンがどれだけ「it_」トークンに関連しているかを計算するとき、2 番目の列で表される 2 番目の注意ヘッドは最初の 2 行、つまりトークン「The」と「animal」に最も焦点を当てているのに対し、3 番目の列は下の 2 行、つまり 2 つのトークンにトークン化された「tired」に最も焦点を当てていることに注意してください。
コンテキスト ウィンドウの範囲内でどのトークンが相互に関連しているかを調べるために、アテンション メカニズムは、それぞれが独自のソフト ウェイトを計算するための独自の「関連性」を持つ複数のアテンション ヘッドを使用して、各トークン、より正確にはその埋め込みの「ソフト」ウェイトを計算します。たとえば、小規模 (つまり、パラメータ サイズが 117M) GPT-2 モデルには、12 個のアテンション ヘッドとわずか 1,000 個のトークンのコンテキスト ウィンドウがありました。[41]中バージョンでは、3 億 4,500 万のパラメータがあり、それぞれ 12 個のアテンション ヘッドを持つ 24 個のレイヤーが含まれています。勾配降下によるトレーニングでは、バッチ サイズ 512 が使用されました [25]。

2024 年 2 月に発表された Google の Gemini 1.5 などの最大のモデルは、最大 100 万のサイズのコンテキスト ウィンドウを持つことができます (1,000 万のコンテキスト ウィンドウも「テストに成功」しました)。大きなコンテキスト ウィンドウを備えた他のモデルには、最大 200,000 トークンのコンテキスト ウィンドウを備えた Anthropic の Claude 2.1 があります。[43]この最大値は入力トークンの数を指し、出力トークンの最大数は入力とは異なり、多くの場合これより小さくなることに注意してください。たとえば、GPT-4 Turbo モデルの最大出力は 4096 トークンです。[44]

モデルが次の回答を生成するときに考慮できる会話の長さは、コンテキスト ウィンドウのサイズによっても制限されます。 ChatGPT などの会話の長さがコンテキスト ウィンドウより長い場合、次の回答を生成するときにコンテキスト ウィンドウ内の部分のみが考慮されるか、モデルは会話の遠すぎる部分を要約するために何らかのアルゴリズムを適用する必要があります。

コンテキスト ウィンドウを大きくすると、計算コストが高くなり、ローカル コンテキストへの焦点が薄れる可能性があるという欠点があり、コンテキスト ウィンドウを小さくすると、モデルが重要な長期依存関係を見逃す可能性があります。それらのバランスをとることは、実験とドメイン固有の考慮事項の問題です。

モデルは、トレーニング データセットからのセグメントが与えられた場合に、セグメントがどのように続くか、またはセグメントに何が欠けているかを予測するために事前トレーニングできます。[45]どちらでも構いません

自己回帰 (つまり、セグメントがどのように続くかを予測する、GPT が行う方法): たとえば、「食べるのが好き」というセグメントが与えられた場合、モデルは「アイスクリーム」または「寿司」を予測します。
「マスク」 (つまり、「BERT」[46] が行う方法で、セグメントから欠落している部分を埋める): たとえば、「クリームを [__] [__] するのが好きです」というセグメントが与えられた場合、モデルは「食べる」と「アイス」が欠落していると予測します。
モデルは、次の文の予測 (NSP) など、データ分布の理解をテストする補助タスクでトレーニングできます。このタスクでは、文のペアが提示され、モデルはそれらがトレーニング コーパス内に連続して出現するかどうかを予測する必要があります。[46]トレーニング中、正則化損失はトレーニングを安定させるためにも使用されます。ただし、正則化損失は通常、テストや評価中には使用されません。

インフラストラクチャー
最大のモデルをトレーニングするには、実質的なインフラストラクチャが必要です。[47][48][49]

研修費用

ソフトウェアとハードウェアの進歩により、2020 年以降コストが大幅に削減されました。そのため、2023 年には 120 億パラメータの LLM のトレーニングの計算コストは 72,300 A100-GPU 時間でしたが、2020 年の時点では 15 億パラメータの LLM のトレーニング コスト (2020 年の最新技術より 2 桁小さい) は、 8万ドルと160万ドル。[50][51][52] 2020年以降、ますます大型モデルに多額の投資が行われてきました。たとえば、2019 年の GPT-2 (つまり 15 億パラメータ モデル) のトレーニングには 50,000 ドルの費用がかかりましたが、2022 年の PaLM (つまり 5,400 億パラメータ モデル) のトレーニングには 800 万ドルがかかり、Megatron-Turing NLG 530B (2021 年) の費用は約 1,100 万ドルでした[53]。

Transformer ベースの LLM の場合、トレーニング コストは推論コストよりもはるかに高くなります。 1 つのトークンでトレーニングするにはパラメーターあたり 6 FLOP のコストがかかりますが、1 つのトークンで推論するにはパラメーターあたり 1 ～ 2 FLOP のコストがかかります。[54]

ツールの使用
原則として、少なくとも外部ツールや追加のソフトウェアを使用しない限り、LLM によっては解決できない特定のタスクがあります。このようなタスクの例は、LLM がトレーニング コーパス内でこの計算の継続にまだ遭遇していない場合に、ユーザーの入力 '354 * 139 = ' に応答することです。このような場合、LLM は結果を計算するプログラム コードを実行する必要があり、その結果を応答に含めることができます。別の例は、「今何時ですか?」これは ' であり、別のプログラム インタプリタがコンピュータ上のシステム時刻を取得するコードを実行する必要があるため、LLM はそれを応答に含めることができます。[55][56]この基本戦略は、生成されたプログラムを複数回試行したり、他のサンプリング戦略を使用したりすることで洗練させることができます。[57]

一般に、LLM でツールを使用できるようにするには、ツールを使用できるように LLM を微調整する必要があります。ツールの数が有限である場合、微調整は 1 回だけ行うことができます。オンライン API サービスのように、ツールの数が任意に増加する可能性がある場合は、LLM を微調整して、API ドキュメントを読み取って API を正しく呼び出すことができます [58][59]。

ツールの使用のより単純な形式は、検索拡張生成、つまり文書検索による LLM の拡張です。クエリが与えられると、ドキュメント取得プログラムが呼び出され、最も関連性の高いドキュメントが取得されます。これは通常、クエリとドキュメントをベクトルにエンコードし、クエリのベクトルに最も類似したベクトルを持つドキュメント (通常はベクトル データベースに保存されている) を検索することによって行われます。その後、LLM は、取得したドキュメントに含まれるクエリとコンテキストの両方に基づいて出力を生成します。[60]

代理店
LLM は言語モデルであり、目標がないためエージェントではありませんが、インテリジェント エージェントのコンポーネントとして使用できます。[61]研究者らは、そのような統合のためのいくつかの方法を説明しています。[要出典]

「Reason + Act」のかばん語である ReAct パターンは、LLM をプランナーとして使用して、LLM からエージェントを構築します。 LLM は「大声で考える」ように促されます。具体的には、言語モデルには、環境のテキストによる説明、目標、可能なアクションのリスト、およびこれまでのアクションと観察の記録が求められます。アクションを生成する前に 1 つ以上の思考を生成し、それが環境内で実行されます。[62] LLM プランナーに与えられる環境の言語記述は、環境を記述する論文の LaTeX コードである場合もあります。[63]

DEPS (「記述、説明、計画、選択」) メソッドでは、LLM は最初に画像の説明を通じて視覚的な世界に接続され、その後、事前に訓練された知識と受け取った環境フィードバックに基づいて、複雑なタスクと行動の計画を作成するように求められます。[64]

Reflexion メソッド [65] は、複数のエピソードにわたって学習するエージェントを構築します。各エピソードの終わりに、LLM にはエピソードの記録が与えられ、次のエピソードでより良いパフォーマンスを発揮するのに役立つ「教訓」を考えるように促されます。これらの「学んだ教訓」は、その後のエピソードでエージェントに与えられます。[要出典]

モンテカルロ ツリー検索では、ロールアウト ヒューリスティックとして LLM を使用できます。プログラムによる世界モデルが利用できない場合、LLM は世界モデルとして機能する環境の説明を求めることもできます。[66]

オープンエンド探索の場合、LLM を使用して観測値の「面白さ」をスコアリングできます。これは、通常の (非 LLM) 強化学習エージェントをガイドするための報酬信号として使用できます。 [67]あるいは、カリキュラム学習のためにますます困難になるタスクを提案することもできます。[68] LLM プランナーは、個々のアクションを出力する代わりに、「スキル」、つまり複雑なアクション シーケンスの関数を構築することもできます。スキルは保存して後で呼び出すことができるため、計画の抽象化レベルを高めることができます。[68]

LLM を利用したエージェントは、以前のコンテキストの長期メモリを保持でき、そのメモリは取得拡張生成と同じ方法で取得できます。複数のそのようなエージェントは社会的に相互作用することができます。[69]

圧縮
通常、LLM は単精度または半精度の浮動小数点数 (float32 および float16) を使用してトレーニングされます。 1 つの float16 は 16 ビット、つまり 2 バイトなので、10 億のパラメーターには 2 GB が必要です。最大のモデルには通常 1,000 億個のパラメータがあり、ロードするのに 200 ギガバイトが必要となるため、ほとんどの家庭用電化製品の範囲外になります [70]。

トレーニング後の量子化[71]は、パフォーマンスのほとんどを維持しながら、トレーニングされたモデルのパラメーターの精度を下げることで、必要なスペースを削減することを目的としています。[72][73]量子化の最も単純な形式は、すべての数値を指定されたビット数に切り捨てるだけです。これは、レイヤーごとに異なる量子化コードブックを使用することで改善できます。さまざまなパラメータにさまざまな精度を適用し、特に重要なパラメータ (「異常値の重み」) についてはより高い精度を適用することで、さらなる改善を行うことができます。[74]視覚的なガイドについては、[75] を参照してください。

通常、量子化モデルはフリーズされ、事前量子化モデルのみが微調整されますが、量子化モデルも引き続き微調整できます。[76]

マルチモダリティ
参照: マルチモーダル学習
マルチモダリティとは「複数のモダリティを持つ」という意味であり、「モダリティ」とはビデオ、画像、オーディオ、テキスト、固有受容などの入力または出力のタイプを指します[77]。画像からラベルへの AlexNet、画像とテキストからテキストへの視覚的質問応答[79]、音声からテキストへの音声認識など、1 つのモダリティを取り込んで別のモダリティを出力するように特別にトレーニングされた AI モデルが数多くあります。

LLM からマルチモーダル モデルを作成する一般的な方法は、トレーニングされたエンコーダーの出力を「トークン化」することです。具体的には、次のように画像を理解できる LLM を構築できます: トレーニングされた LLM を取得し、トレーニングされた画像エンコーダーを取得します。 
E
{\displaystyle E}。小さな多層パーセプトロンを作成する 
f
{\displaystyle f} なので、どの画像に対しても 
y
{\displaystyle y}、後処理されたベクトル 
f
(
E
(
y
)
)
{\displaystyle f(E(y))} は、エンコードされたトークンと同じ次元を持ちます。それが「イメージトークン」です。次に、テキスト トークンと画像トークンをインターリーブできます。次に、複合モデルは画像とテキストのデータセットで微調整されます。この基本構造をさらに洗練して適用してモデルを改善できます。安定性を向上させるために画像エンコーダーがフリーズされる場合があります。[80]

Flamingo は、事前トレーニング済みの言語モデルと画像エンコーダーのペアを微調整して、ゼロからトレーニングしたモデルよりも視覚的な質問応答のパフォーマンスが向上するトークン化手法の有効性を実証しました。[81] Google PaLM モデルはトークン化手法を使用してマルチモーダル モデル PaLM-E に微調整され、ロボット制御に適用されました [82]。 LLaMA モデルもトークン化手法を使用してマルチモーダルになり、画像入力 [83] やビデオ入力 [84] が可能になりました。

GPT-4 はテキストと画像の両方を入力として使用できます[85] (ただし、ビジョン コンポーネントは GPT-4V まで公開されませんでした[86])。 Google DeepMind の Gemini もマルチモーダルです。[87]

プロパティ
スケーリングの法則
詳細は「ニューラル スケーリングの法則」を参照
次の 4 つのハイパーパラメータは LLM を特徴づけます。

(事前)トレーニングの費用 (
C
{\displaystyle C})、
人工ニューラルネットワーク自体のサイズ（パラメータ数など） 
N
{\displaystyle N} (つまり、その層内のニューロンの量、それらの間の重みの量、およびバイアス)、
(事前) トレーニング データセットのサイズ (つまり、コーパス内のトークンの数、 
D
{\displaystyle D})、
（事前）トレーニング後のパフォーマンス。
それらは、「スケーリング則」と呼ばれる単純な統計法則によって関連付けられています。対数対対数学習率スケジュールを使用して、1 エポックに対して自己回帰的にトレーニングされた LLM の特定のスケーリング則 (「チンチラ スケーリング」) では、次のように述べられています。[88]
{
C
=
C
0
N
D
L
=
あ
N
α
+
B
D
β
+
L
0
{\displaystyle {\begin{cases}C=C_{0}ND\\[6pt]L={\frac {A}{N^{\alpha }}}+{\frac {B}{D^{\beta }}}+L_{0}\end{cases}}} ここで、変数は

C
{\displaystyle C} は、モデルのトレーニングにかかるコスト (FLOP 単位) です。
N
{\displaystyle N} はモデル内のパラメータの数です。
D
{\displaystyle D} はトレーニング セット内のトークンの数です。
L
{\displaystyle L} は、テスト データセットでトレーニングされた LLM によって達成される、トークンあたりの平均負の対数尤度損失 (nats/トークン) です。
統計的ハイパーパラメータは次のとおりです。

C
0
=
6
{\displaystyle C_{0}=6}。これは、1 つのトークンでトレーニングするのにパラメーターごとに 6 FLOP のコストがかかることを意味します。トレーニング コストは推論コストよりはるかに高いことに注意してください。1 つのトークンを推論するにはパラメーターごとに 1 ～ 2 FLOP のコストがかかります。[54]
α
=
0.34
、
β
=
0.28
、
あ
=
406.4
、
B
=
410.7
、
L
0
=
1.69
{\displaystyle \alpha =0.34,\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}
創発的な能力


At point(s) referred to as breaks,[89] the lines change their slopes, appearing on a linear-log plot as a series of linear segments connected by arcs.
Performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by "break(s)"[89] in the scaling law, where the slope of the line changes abruptly, and where larger models acquire "emergent abilities".[39][90]これらはモデルのコンポーネントの複雑な相互作用から発生し、明示的にプログラムまたは設計されたものではありません。[2]

創発的な能力の中で最も興味深いのは、実例のデモンストレーションからの文脈内学習です。[91]コンテキスト内学習には、次のようなタスクが含まれます。

報告された算術、国際音声文字の解読、単語の文字のスクランブルの解除、文脈内の単語の曖昧さの解消、空間単語の変換、基本的な方向（たとえば、[0, 0, 1; 0, 0, 0; 0, 0, 0] に対して「北東」と応答する）、テキストで表現される色用語 [94]。
思考連鎖プロンプト: モデル サイズが 62B を超える場合にのみ、思考連鎖プロンプトによってモデルの出力が改善されます。小規模なモデルは、思考連鎖を行わずに即座に回答を求められた場合にパフォーマンスが向上します。[95]
ヒンディー語（ヒンディー語と英語の組み合わせ）の段落内の攻撃的な内容を特定し、スワヒリ語のことわざに相当する同様の英語を生成します。 [96]
シェーファーら。アル。創発的な能力は予期せぬ形で獲得されるのではなく、滑らかなスケーリング則に従って予測通りに獲得されると主張する。著者らは、多肢選択問題を解くLLMのおもちゃの統計モデルを検討し、他のタイプのタスクを考慮して修正されたこの統計モデルが、これらのタスクにも同様に適用されることを示しました。

しましょう 
×
{\displaystyle x} はパラメータ数であり、 
y
{\displaystyle y} はモデルのパフォーマンスになります。

いつ 
y
=
平均的な 
広報
(
正しいトークン
)
{\displaystyle y={\text{平均 }}\Pr({\text{正しいトークン}})}、その後 
(
ログ
⁡
×
、
y
)
{\displaystyle (\log x,y)} は指数曲線 (1 つのプラトーに達する前) であり、創発のように見えます。
いつ 
y
=
平均的な 
ログ
⁡
(
広報
(
正しいトークン
)
)
{\displaystyle y={\text{平均 }}\log(\Pr({\text{正しいトークン}}))}、その後、 
(
ログ
⁡
×
、
y
)
{\displaystyle (\log x,y)} プロットは直線 (ゼロでプラトーに達する前) であり、創発のようには見えません。
いつ 
y
=
平均的な 
広報
(
最も可能性の高いトークンが正しいです
)
{\displaystyle y={\text{平均 }}\Pr({\text{最も可能性の高いトークンが正しい}})}、その後 
(
ログ
⁡
×
、
y
)
{\displaystyle (\log x,y)} はステップ関数であり、創発のように見えます。
解釈
大規模な言語モデル自体は「ブラック ボックス」であり、言語モデルがどのように言語タスクを実行できるかは不明です。 LLM の仕組みを理解するには、いくつかの方法があります。

機械的解釈可能性は、LLM によって実行される推論を近似する記号アルゴリズムを発見することにより、LLM をリバース エンジニアリングすることを目的としています。 1 つの例は Othello-GPT です。ここでは、小さな Transformer がオセロの正当な手を予測するように訓練されています。オセロ盤には線形表現が存在し、その表現を変更すると、予測される合法的なオセロの動きが正しい方法で変化することが判明した[98][99]。別の例では、小さな Transformer が Karel プログラムでトレーニングされます。 Othello-GPT の例と同様に、Karel プログラム セマンティクスの線形表現があり、その表現を変更すると出力が正しい方法で変更されます。このモデルは、トレーニング セット内のプログラムよりも平均して短い正しいプログラムも生成します。[100]

別の例では、著者らはモジュラー算術加算に関して小さな変換器を訓練しました。結果として得られたモデルをリバース エンジニアリングしたところ、離散フーリエ変換が使用されていることが判明しました。 [101]

理解力と知性
2022年の調査で、（調整されていない）LLMが「自明ではない意味で自然言語を（これまでに）理解できるか」という質問に対し、NLP研究者の意見は真っ二つに分かれた。 [102] 「LLM 理解」の支持者は、数学的推論などの一部の LLM 能力は、特定の概念を「理解する」能力を意味すると信じています。マイクロソフトチームは2023年に、GPT-4は「数学、コーディング、ビジョン、医学、法律、心理学などにまたがる斬新で困難なタスクを解決できる」と主張し、GPT-4は「汎用人工知能システムの初期の(まだ不完全な)バージョンと考えるのが合理的である」と主張した。「ソフトウェアエンジニアリング候補者の試験に合格するシステムが、実際にはインテリジェントではないと合理的に言えるだろうか?」 [103] [104] 一部の研究者はLLMを「エイリアン」と特徴づけているインテリジェンス」。[105][106]たとえば、Conjecture CEO の Connor Leahy 氏は、チューニングされていない LLM は不可解なエイリアン「ショゴス」のようなものだと考えており、RLHF のチューニングは LLM の内部動作を隠す「笑顔のファサード」を作り出すと信じています。人間以外の理解力。」[107][108]

対照的に、「LLMは理解力に欠けている」学派の支持者の中には、既存のLLMが「単に既存の文章をリミックスして再結合しているだけ」であると信じている[106]、これは確率論的オウムとして知られる現象であるか、または既存のLLMが予測スキル、推論スキル、主体性、説明可能性において引き続き欠陥を抱えていることを指摘している[102]。たとえば、GPT-4 には計画とリアルタイム学習において自然な欠陥があります。[104]生成LLMは、トレーニングデータによって正当化されないと思われる事実の主張を自信を持って主張することが観察されており、この現象は「幻覚」と呼ばれています[109]。具体的には、LLM の文脈における幻覚は、構文的に健全で、流暢で自然に見えるが、事実としては不正確、無意味、または提供されたソース入力に忠実ではないテキストまたは応答の生成に対応します。 [110]神経科学者のテレンス・セジノウスキーは、「LLMの知能に関する専門家の意見の相違は、自然知能に基づく私たちの古い考えが不十分であることを示唆している」と主張した[102]。

LLM が示す知性や理解を示す問題には、主に 2 つの側面があります。1 つ目は、コンピューター システム内で思考と言語をモデル化する方法であり、2 つ目は、コンピューター システムが人間のような言語を生成できるようにする方法です。[102]認知のモデルとしての言語のこれらの側面は、認知言語学の分野で開発されてきました。アメリカの言語学者ジョージ・レイコフは、言語を学習課題と理解のモデルとして使用するための計算基盤として、言語の神経理論(NTL)[111]を提示しました。 NTL モデルは、人間の脳の特定の神経構造が思考と言語の性質をどのように形成するか、そしてコンピューター システムで思考と言語をモデル化するために適用できる、そのような神経システムの計算特性がどのようなものであるかを概説します。コンピュータ システムで言語をモデル化するためのフレームワークが確立された後、焦点は許容可能な文法を備えた言語を生成するためのコンピュータ システムのフレームワークを確立することに移りました。英国の認知言語学者でありデジタルコミュニケーション技術者であるヴィヴィアン・エヴァンスは、2014 年の著書『言語神話: なぜ言語は本能ではないのか』の中で、NLP による認知パターンのモデル化と人間のような言語の生成を可能にする確率的文脈自由文法 (PCFG) の役割を詳しく説明しました。[112][113]

評価
困惑
言語モデルのパフォーマンスの最も一般的に使用される尺度は、特定のテキスト コーパスにおける言語モデルの複雑さです。パープレキシティは、モデルがデータセットの内容をどの程度正確に予測できるかを示す尺度です。モデルがデータセットに割り当てる尤度が高いほど、複雑さは低くなります。数学的には、パープレキシティは、トークンごとの平均負対数尤度の指数関数として定義されます。
ログ
⁡
(
困惑
)
=
−
1
N
∑
私は
=
1
N
ログ
⁡
(
広報
(
トークン
私は
∣
トークンのコンテキスト
私は
)
)
{\displaystyle \log({\text{Perplexity}})=-{\frac {1}{N}}\sum _{i=1}^{N}\log(\Pr({\text{トークン}}_{i}\mid {\text{トークンのコンテキスト}}_{i}))}ここ 
N
{\displaystyle N} はテキスト コーパス内のトークンの数であり、「トークンのコンテキスト」 
私は
{\displaystyle i}" は、使用される LLM の特定のタイプによって異なります。 LLM が自己回帰の場合、「トークンのコンテキスト」 
私は
{\displaystyle i}" は、トークンの前に表示されるテキストのセグメントです 
私は
{\displaystyle i}。 LLM がマスクされている場合、「トークンのコンテキスト」 
私は
{\displaystyle i}" はトークンを囲むテキストのセグメントです 
私は
{\displaystyle i}。

言語モデルはトレーニング データに過剰適合する可能性があるため、モデルは通常、目に見えないデータのテスト セットでの複雑さによって評価されます。これは、大規模な言語モデルの評価に特有の課題をもたらします。主に Web から収集されたテキストのますます大規模なコーパスでトレーニングされるため、モデルのトレーニング データに特定のテスト セットの一部が誤って含まれる可能性がますます高まっています。

BPW、BPC、および BPT
情報理論では、エントロピーの概念は複雑さと複雑に関連しており、この関係は特にクロード・シャノンによって確立されました。この関係は数学的に次のように表されます。 
エントロピー
=
ログ
2
⁡
(
困惑
)
{\displaystyle {\text{エントロピー}}=\log _{2}({\text{パープレキシティ}})}。

この文脈におけるエントロピーは、通常、単語あたりのビット数 (BPW) または文字あたりのビット数 (BPC) の観点から定量化されます。これは、言語モデルが単語ベースのトークン化を使用するか文字ベースのトークン化を使用するかによって異なります。

特に、主にサブワードトークン化を使用する大規模な言語モデルの場合、トークンあたりのビット数 (BPT) が、一見より適切な尺度として浮上します。ただし、異なる大規模言語モデル (LLM) 間ではトークン化方法が異なるため、BPT は多様なモデル間の比較分析の信頼できる指標として機能しません。 BPT を BPW に変換するには、単語ごとの平均トークン数を乗算します。

言語モデルの評価と比較では、一般にクロスエントロピーがエントロピーよりも優先されるメトリックです。基本的な原則は、BPW が低いほど、モデルの圧縮能力が強化されていることを示すということです。これは、正確な予測を行う際のモデルの習熟度を反映しています。

タスク固有のデータセットとベンチマーク
より具体的な下流タスクにおける言語モデルの機能を評価するために、多数のテスト データセットとベンチマークも開発されています。テストは、一般知識、常識的推論、数学的問題解決など、さまざまな能力を評価するように設計されている場合があります。

評価データセットの広範なカテゴリの 1 つは、質問と正解のペアで構成される質問応答データセットです (たとえば、「サンノゼ シャークスはスタンレー カップで優勝しましたか?」、「いいえ」)。[115]モデルのプロンプトに、期待される答えを導き出すことができるテキストが含まれている場合、質問応答タスクは「オープンブック」とみなされます (たとえば、前の質問に、「シャークスはスタンレーカップ決勝に一度進出しましたが、2016 年にピッツバーグ・ペンギンズに負けました。」という文を含むテキストが隣接している可能性があります。[115])。それ以外の場合、タスクは「クローズドブック」とみなされ、モデルはトレーニング中に保持された知識を利用する必要があります。[116]一般的に使用される質問応答データセットの例としては、TruthfulQA、Web question、TriviaQA、SQuAD などがあります。[116]

評価データセットはテキスト補完の形式をとることもあり、モデルに最も可能性の高い単語または文を選択させてプロンプトを完成させることもできます。たとえば、「アリスはボブと友達でした。アリスは友人の ____ に会いに行きました。」[3]。

さまざまな評価データセットとタスクを組み合わせた複合ベンチマークもいくつか開発されています。例としては、GLUE、SuperGLUE、MMLU、BIG-bench、HELM などがあります。[114][116] OpenAI は複合ベンチマークを実行するためのツールをリリースしましたが、評価結果はプロンプト方法の影響を受けやすいと指摘しました。[117][118]一部の公開データセットには、ラベルが間違っている、曖昧である、回答できない、またはその他の低品質の質問が含まれており、これらをクリーンアップして、より信頼性の高いベンチマーク スコアを得ることができます。 [119]

以前は、残りの部分で教師付き微調整を行った後、評価データセットの保留部分の結果を報告するのが標準でした。現在では、プロンプト手法を通じて事前トレーニング済みモデルを直接評価することがより一般的になっていますが、特定のタスクのプロンプトを作成する方法の詳細、特にプロンプ​​トに解決済みタスクの例がいくつ隣接しているか (つまり、n ショット プロンプトの n の値) に関しては研究者によって異なります。

敵対的に構築された評価
大規模な言語モデルの改善ペースが速いため、評価ベンチマークの寿命が短くなり、最先端のモデルが既存のベンチマークをすぐに「飽和」させ、ヒューマン・アノテーターのパフォーマンスを超え、より困難なタスクでベンチマークを置き換えたり、強化したりする取り組みが行われています[120]。さらに、AI が、実際に問われている質問を必ずしも理解することなく、正しい応答を推測するために、表面的なテスト質問の文言の統計的相関を使用することにより、多肢選択テストで「カンニング」する「ショートカット学習」のケースもあります [102]。

一部のデータセットは、既存の言語モデルのパフォーマンスが人間に比べて異常に低いと思われる特定の問題に焦点を当てて、敵対的に構築されています。 1 つの例は、TruthfulQA データセットです。これは 817 の質問で構成される質問応答データセットで、言語モデルは、トレーニング中に繰り返し暴露された虚偽を模倣することで誤答する可能性があります。たとえば、LLM は「年老いた犬に新しい芸を教えられますか?」という質問に「いいえ」と答えるかもしれません。英語の慣用句にさらされているため、たとえ文字通りに真実ではないとしても、年老いた犬に新しい芸を教えることはできません。 [121]

敵対的評価データセットのもう 1 つの例は、Swag とその後継である HellaSwag です。これは、テキストの一節を完了するために複数のオプションの 1 つを選択する必要がある問題のコレクションです。誤った補完は、言語モデルからサンプリングし、一連の分類子でフィルタリングすることによって生成されました。結果として生じる問題は人間にとっては些細なものですが、データセットが作成された当時、最先端の言語モデルの精度は低かったです。例えば：

フィットネスセンターの看板が見えます。次に、カメラに向かって話し、バランスボールの上に座ったり横たわったりしている男性の姿が見えます。男は...
a) ボールを上下に走らせることで効率的な運動量を増やす方法を示します。
b) 腕と脚をすべて動かし、筋肉をたくさん鍛えます。
c) 次にボールをプレーすると、グラフィックスと生垣トリミングのデモンストレーションが表示されます。
d) ボールの上で話しながら腹筋運動をする。[122]

BERT は b) を最も可能性の高い完了として選択しますが、正解は d) です。 [122]

より広範囲な影響
2023年、Nature Biomedical Engineeringは、人間が書いたテキストと大規模な言語モデルによって作成されたテキストを「正確に区別することはもはや不可能」であり、「汎用の大規模言語モデルが急速に普及することはほぼ確実である...それらが時間の経過とともに多くの業界を変えることはかなり安全な賭けである。」 [123] ゴールドマン・サックスは2023年に、生成言語AIは今後10年間で世界のGDPを7%増加させる可能性があり、自動化の危険にさらされる可能性があると示唆した世界中で 3 億の雇用。[124][125]

記憶と著作権
詳細情報: 人工知能と著作権
記憶は LLM における新たな動作であり、従来の人工ニューラル ネットワークの典型的な動作とは対照的に、長いテキスト文字列がトレーニング データから逐語的に出力されることがあります。制御された LLM 出力の評価では、トレーニング データ (GPT-2 シリーズ モデルに焦点を当てた) から記憶される量が、正確な重複の場合は 1% 以上 [126]、最大約 7% までさまざまです。

セキュリティ
一部のコメント投稿者は、誤った情報やその他の形態の誤用が偶発的または意図的に作成されることに対して懸念を表明した。たとえば、大規模な言語モデルが利用できるようになると、バイオテロを実行するために必要なスキル レベルが低下する可能性があります。バイオセキュリティ研究者のケビン・エスベルトは、LLM作成者は病原体の作成または強化に関するトレーニングデータペーパーを除外すべきであると提案した[129]。

Google と、コーネル大学やカリフォルニア大学バークレー校を含むいくつかの大学の研究者らによる研究では、ChatGPT などの言語モデルには潜在的なセキュリティ リスクがあることが示されました。研究では、質問者が AI モデルが使用したトレーニング データを ChatGPT から取得できる可能性を調査し、確認しました。たとえば、ChatGPT 3.5 ターボに「ポエム」という単語を永遠に繰り返すように依頼すると、AI モデルは「ポエム」と何百回も言ってから発散し、標準的な対話形式から逸脱して意味のないフレーズを吐き出すため、トレーニング データがそのまま吐き出されます。研究者らは、同様の方法でトレーニング データを公開する AI モデルの例を 10,000 件以上見てきました。研究者らは、AIモデルが実際に安全かどうかを判断するのは難しいと述べた[130]。

LLM モデル内に「スリーパー エージェント」が存在する可能性があることも、新たなセキュリティ上の懸念事項です。これらはモデルに組み込まれた隠れた機能であり、特定のイベントまたは条件によってトリガーされるまで休止状態になります。アクティブ化すると、LLM は予期された動作から逸脱し、安全でないアクションを実行します。[131]

ChatGPT や Claude など、一般に公開されている大規模言語モデル (LLM) アプリケーションには、通常、有害なコンテンツをフィルタリングするように設計された安全対策が組み込まれています。ただし、これらの制御を効果的に実装することは困難であることが判明しています。たとえば、Kang らによる研究。 [132] は、LLM 安全システムを回避する方法を実証しました。同様に、Wang [133] は、潜在的な犯罪者がどのようにして ChatGPT 4o の安全制御をバイパスして麻薬密売活動の確立に関する情報を入手することができるかを説明しました。

