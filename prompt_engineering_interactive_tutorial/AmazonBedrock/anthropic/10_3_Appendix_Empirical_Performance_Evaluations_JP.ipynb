{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating AI Models: Code, Human, and Model-Based Grading\n",
        "\n",
        "（AIモデルの評価：コード採点・人手採点・モデル採点）\n",
        "\n",
        "このノートブックでは、Claude v3 のような AI モデルの有効性を評価するために広く使われている 3 つの手法を扱います。\n",
        "\n",
        "1. コードによる採点（Code-based grading）\n",
        "2. 人手による採点（Human grading）\n",
        "3. モデルによる採点（Model-based grading）\n",
        "\n",
        "それぞれを例で示し、AI の性能評価における利点と限界を確認します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code-Based Grading Example: Sentiment Analysis\n",
        "\n",
        "この例では、映画レビューの感情（ポジティブ／ネガティブ）を分類する Claude の能力を評価します。モデルの出力が期待されるラベルと一致しているかを、コードでチェックできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Store the model name and AWS region for later use\n",
        "MODEL_NAME = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
        "AWS_REGION = \"us-west-2\"\n",
        "\n",
        "%store MODEL_NAME\n",
        "%store AWS_REGION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Install the Anthropic package\n",
        "%pip install anthropic --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import the AnthropicBedrock class and create a client instance\n",
        "from anthropic import AnthropicBedrock\n",
        "\n",
        "client = AnthropicBedrock(aws_region=AWS_REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for sentiment analysis\n",
        "def build_input_prompt(review):\n",
        "    user_content = f\"\"\"Classify the sentiment of the following movie review as either 'positive' or 'negative' provide only one of those two choices:\n",
        "    <review>{review}</review>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"review\": \"This movie was amazing! The acting was superb and the plot kept me engaged from start to finish.\",\n",
        "        \"golden_answer\": \"positive\"\n",
        "    },\n",
        "    {\n",
        "        \"review\": \"I was thoroughly disappointed by this film. The pacing was slow and the characters were one-dimensional.\",\n",
        "        \"golden_answer\": \"negative\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Function to get completions from the model\n",
        "def get_completion(messages):\n",
        "    message = client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2000,\n",
        "        temperature=0.0,\n",
        "        messages=messages\n",
        "    )\n",
        "    return message.content[0].text\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"review\"])) for item in eval]\n",
        "\n",
        "# Print the outputs and golden answers\n",
        "for output, question in zip(outputs, eval):\n",
        "    print(f\"Review: {question['review']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")\n",
        "\n",
        "# Function to grade the completions\n",
        "def grade_completion(output, golden_answer):\n",
        "    return output.lower() == golden_answer.lower()\n",
        "\n",
        "# Grade the completions and print the accuracy\n",
        "grades = [grade_completion(output, item[\"golden_answer\"]) for output, item in zip(outputs, eval)]\n",
        "print(f\"Accuracy: {sum(grades) / len(grades) * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Human Grading Example: Essay Scoring\n",
        "\n",
        "エッセイの採点のように、コードだけでは評価しにくいタスクもあります。その場合は、人間の評価者がモデル出力を評価できるように、採点ガイドライン（基準）を用意します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for essay generation\n",
        "def build_input_prompt(topic):\n",
        "    user_content = f\"\"\"Write a short essay discussing the following topic:\n",
        "    <topic>{topic}</topic>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"topic\": \"The importance of education in personal development and societal progress\",\n",
        "        \"golden_answer\": \"A high-scoring essay should have a clear thesis, well-structured paragraphs, and persuasive examples discussing how education contributes to individual growth and broader societal advancement.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"topic\"])) for item in eval]\n",
        "\n",
        "# Print the outputs and golden answers\n",
        "for output, item in zip(outputs, eval):\n",
        "    print(f\"Topic: {item['topic']}\\n\\nGrading Rubric:\\n {item['golden_answer']}\\n\\nModel Output:\\n{output}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model-Based Grading Examples\n",
        "\n",
        "モデルの回答と採点ルーブリック（評価基準）を与えることで、Claude 自身に自分の出力を採点させることもできます。これにより、本来は人間の判断が必要になりがちなタスクの評価を自動化できます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Summarization\n",
        "\n",
        "この例では、Claude が生成した要約の品質を Claude 自身に評価させます。長文から重要情報を簡潔かつ正確に抜き出せているかを評価したいときに有用です。含めるべき要点をまとめたルーブリックを与えることで、採点プロセスを自動化し、要約タスクにおけるモデル性能を素早く確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for summarization\n",
        "def build_input_prompt(text):\n",
        "    user_content = f\"\"\"Please summarize the main points of the following text:\n",
        "    <text>{text}</text>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Function to build the grader prompt for assessing summary quality\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"Assess the quality of the following summary based on this rubric:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <summary>{output}</summary>\n",
        "    Provide a score from 1-5, where 1 is poor and 5 is excellent.\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"text\": \"The Magna Carta, signed in 1215, was a pivotal document in English history. It limited the powers of the monarchy and established the principle that everyone, including the king, was subject to the law. This laid the foundation for constitutional governance and the rule of law in England and influenced legal systems worldwide.\",\n",
        "        \"golden_answer\": \"A high-quality summary should concisely capture the key points: 1) The Magna Carta's significance in English history, 2) Its role in limiting monarchical power, 3) Establishing the principle of rule of law, and 4) Its influence on legal systems around the world.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
        "\n",
        "# Grade the completions\n",
        "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
        "\n",
        "# Print the summary quality score\n",
        "print(f\"Summary quality score: {grades[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Fact-Checking\n",
        "\n",
        "この例では、主張のファクトチェックを Claude に行わせ、そのファクトチェック自体の正確さを評価します。正確な情報と不正確な情報を区別する能力を評価したい場合に有用です。正しいファクトチェックに必要な要点をルーブリックとして与えることで、採点を自動化し、ファクトチェックタスクにおけるモデル性能を素早く確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for fact-checking\n",
        "def build_input_prompt(claim):\n",
        "    user_content = f\"\"\"Determine if the following claim is true or false:\n",
        "    <claim>{claim}</claim>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Function to build the grader prompt for assessing fact-check accuracy\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"Based on the following rubric, assess whether the fact-check is correct:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <fact-check>{output}</fact-check>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"claim\": \"The Great Wall of China is visible from space.\",\n",
        "        \"golden_answer\": \"A correct fact-check should state that this claim is false. While the Great Wall is an impressive structure, it is not visible from space with the naked eye.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"claim\"])) for item in eval]\n",
        "\n",
        "grades = []\n",
        "for output, item in zip(outputs, eval):\n",
        "    # Print the claim, fact-check, and rubric\n",
        "    print(f\"Claim: {item['claim']}\\n\")\n",
        "    print(f\"Fact-check: {output}]\\n\")\n",
        "    print(f\"Rubric: {item['golden_answer']}\\n\")\n",
        "    \n",
        "    # Grade the fact-check\n",
        "    grader_prompt = build_grader_prompt(output, item[\"golden_answer\"])\n",
        "    grade = get_completion(grader_prompt)\n",
        "    grades.append(\"correct\" in grade.lower())\n",
        "\n",
        "# Print the fact-checking accuracy\n",
        "accuracy = sum(grades) / len(grades)\n",
        "print(f\"Fact-checking accuracy: {accuracy * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Tone Analysis\n",
        "\n",
        "この例では、与えられたテキストのトーン（語調・雰囲気）を Claude に分析させ、その分析の正確さを評価します。文章に含まれる感情的な内容や態度を識別し解釈する能力を評価したい場合に有用です。識別すべきトーンの観点をルーブリックとして与えることで、採点を自動化し、トーン分析タスクにおけるモデル性能を素早く確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to build the input prompt for tone analysis\n",
        "def build_input_prompt(text):\n",
        "    user_content = f\"\"\"Analyze the tone of the following text:\n",
        "    <text>{text}</text>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Function to build the grader prompt for assessing tone analysis accuracy\n",
        "def build_grader_prompt(output, rubric):\n",
        "    user_content = f\"\"\"Assess the accuracy of the following tone analysis based on this rubric:\n",
        "    <rubric>{rubric}</rubric>\n",
        "    <tone-analysis>{output}</tone-analysis>\"\"\"\n",
        "    return [{\"role\": \"user\", \"content\": user_content}]\n",
        "\n",
        "# Define the evaluation data\n",
        "eval = [\n",
        "    {\n",
        "        \"text\": \"I can't believe they canceled the event at the last minute. This is completely unacceptable and unprofessional!\",\n",
        "        \"golden_answer\": \"The tone analysis should identify the text as expressing frustration, anger, and disappointment. Key words like 'can't believe', 'last minute', 'unacceptable', and 'unprofessional' indicate strong negative emotions.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get completions for each input\n",
        "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
        "\n",
        "# Grade the completions\n",
        "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
        "\n",
        "# Print the tone analysis quality\n",
        "print(f\"Tone analysis quality: {grades[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これらの例は、コード採点・人手採点・モデル採点が、Claude のような AI モデルをさまざまなタスクで評価するのに使えることを示しています。どの評価方法を選ぶかは、タスクの性質と利用できるリソースによって決まります。モデル採点は、本来は人間の判断が必要となる複雑なタスク評価を自動化する有望なアプローチです。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
