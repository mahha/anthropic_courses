{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e331b3b8-9bda-43b3-a03f-b2e9005765d9",
      "metadata": {},
      "source": [
        "# ストリーミング\n",
        "\n",
        "## 学習目標\n",
        "\n",
        "* ストリーミングの仕組みを理解する\n",
        "* ストリームイベントを扱えるようになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3536ce73",
      "metadata": {},
      "source": [
        "`anthropic` SDK を import して、クライアントをセットアップしましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a681584d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from anthropic import Anthropic\n",
        "\n",
        "#load environment variable\n",
        "load_dotenv()\n",
        "\n",
        "#automatically looks for an \"ANTHROPIC_API_KEY\" environment variable\n",
        "client = Anthropic()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217ecef5",
      "metadata": {},
      "source": [
        "ここまで私たちは、次のような書き方で Claude にメッセージを送ってきました：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470f37aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me an essay about macaws and clay licks in the Amazon\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=800,\n",
        "    temperature=0,\n",
        ")\n",
        "print(\"We have a response back!\")\n",
        "print(\"========================\")\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715293a1",
      "metadata": {},
      "source": [
        "この方法でも問題なく動きますが、このやり方では **すべての内容が生成し終わってから** はじめて API から内容が返ってくる点に注意してください。上のセルをもう一度実行すると、全体の応答が一括で表示されるまで何も出力されないはずです。\n",
        "\n",
        "多くの場面ではこれで十分ですが、ユーザーが「応答が全部生成されるまで待たされる」アプリを作っている場合、体験が悪くなりやすいです。\n",
        "\n",
        "**そこでストリーミングです！**\n",
        "\n",
        "ストリーミングを使うと、モデルが生成した内容を「生成され次第」受け取れるため、全体が完成するのを待つ必要がありません。`claude.ai` のようなアプリはこの仕組みで、生成された内容がブラウザへ順次送られて表示されます：\n",
        "\n",
        "![claude_streaming](images/claude_streaming.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e900e5d2",
      "metadata": {},
      "source": [
        "## ストリームを扱う\n",
        "\n",
        "API からストリーミング応答を得るには、`client.messages.create` に `stream=True` を渡すだけです。ここまでは簡単です。少しややこしいのは、その後にストリーミング応答をどう扱い、流れてくるデータをどう処理するかです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e53041a",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7ea405",
      "metadata": {},
      "source": [
        "`stream` 変数の中身を見てみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf731712",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db973d7e",
      "metadata": {},
      "source": [
        "見た目にはあまり情報がありません！この stream オブジェクト単体では、ほとんど何もしてくれません。stream はジェネレータで、API から受け取った server-sent events (SSE) を 1 つずつ yield します。\n",
        "\n",
        "つまり、こちら側で反復（iterate）して、各 SSE を処理するコードを書く必要があります。もう「完成した 1 つの塊」としてデータが返ってくるのではありません。では実際に stream を回してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2446578",
      "metadata": {},
      "outputs": [],
      "source": [
        "for event in stream:\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33eb9f8b",
      "metadata": {},
      "source": [
        "ご覧の通り、API からは多くの SSE（イベント）が届きます。これらの意味をもう少し詳しく見ていきます。次はイベントを色分けして説明した図です：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0929c678",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "![streaming_output](images/streaming_output.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeba33c0",
      "metadata": {},
      "source": [
        "各ストリームは、次の順序でイベントが流れてきます：\n",
        "\n",
        "* **MessageStartEvent** - content が空の Message\n",
        "* **複数の content block**（各ブロックは次を含む）\n",
        "  * **ContentBlockStartEvent**\n",
        "  * 1 個以上の **ContentBlockDeltaEvent**\n",
        "  * **ContentBlockStopEvent**\n",
        "* 最終メッセージのトップレベルの変化を示す **MessageDeltaEvent**（1個以上）\n",
        "* 最後に **MessageStopEvent**\n",
        "\n",
        "上の例では content block は 1 つだけでした。次の図は、その 1 ブロックに関連するイベント全体を示します：\n",
        "\n",
        "![content_block_streaming](images/content_block_streaming.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e791652f",
      "metadata": {},
      "source": [
        "私たちが本当に欲しい「モデルが生成した本文」は、`ContentBlockDeltaEvent` から届きます（`type` が `\"content_block_delta\"`）。実際のテキストは `delta` の中の `text` にあります。生成されたテキストだけを表示してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c794ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9329b127",
      "metadata": {},
      "source": [
        "テキスト自体は出せていますが、出力が分断されて読みづらいです。Python の `print()` でストリーミングテキストを表示するときは、次の 2 つの引数が便利です：\n",
        "\n",
        "* `end=\"\"`: `print()` はデフォルトで末尾に改行（`\n",
        "`）を付けますが、`end=\"\"` にすると改行しません。次の `print()` が同じ行に続いて表示されます。\n",
        "* `flush=True`: バッファを待たずに即時出力します。ストリーミングで「リアルタイムに表示」したいときに有効です。\n",
        "\n",
        "これを反映してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70beba80",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d90d94",
      "metadata": {},
      "source": [
        "これくらい短いテキストだと、ストリーミングの効果が分かりにくいかもしれません。もう少し長いものを生成させてみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818d04b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do large language models work?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221e30f2",
      "metadata": {},
      "source": [
        "まだなら、上のセルを実行してみてください。テキストが少しずつ（増分で）表示されるはずです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229d1b68",
      "metadata": {},
      "source": [
        "ここまで見た通り、`ContentBlockDeltaEvent` にモデルが生成したテキストが入っています。ただし、他のイベントも重要です。例えばトークン使用量を知りたい場合、見るべき場所は 2 つあります：\n",
        "\n",
        "* `MessageStartEvent` に入力（プロンプト）のトークン使用量が入る\n",
        "* `MessageDeltaEvent` に出力トークン数が入る\n",
        "\n",
        "![streaming_tokens](images/streaming_tokens.png)\n",
        "\n",
        "では、上のコードを更新して、入力トークン数と出力トークン数を表示してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdcab292",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do large language models work?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"message_start\":\n",
        "        input_tokens = event.message.usage.input_tokens\n",
        "        print(\"MESSAGE START EVENT\", flush=True)\n",
        "        print(f\"Input tokens used: {input_tokens}\", flush=True)\n",
        "        print(\"========================\")\n",
        "    elif event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")\n",
        "    elif event.type == \"message_delta\":\n",
        "        output_tokens = event.usage.output_tokens\n",
        "        print(\"\\n========================\", flush=True)\n",
        "        print(\"MESSAGE DELTA EVENT\", flush=True)\n",
        "        print(f\"Output tokens used: {output_tokens}\", flush=True)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea87e85",
      "metadata": {},
      "source": [
        "### その他のストリーミングイベントタイプ\n",
        "\n",
        "ストリームを扱っていると、他にも次のようなイベントに遭遇することがあります：\n",
        "\n",
        "* **Ping イベント** - ストリームには任意個の ping が含まれ得ます。\n",
        "* **Error イベント** - ストリーム内にエラーイベントが混ざることがあります。例えば高負荷時には `overloaded_error` が返ることがあり、非ストリーミングの場合の HTTP 529 に相当します。\n",
        "\n",
        "エラーイベントの例：\n",
        "\n",
        "```\n",
        "event: error\n",
        "data: {\"type\": \"error\", \"error\": {\"type\": \"overloaded_error\", \"message\": \"Overloaded\"}}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6f271d",
      "metadata": {},
      "source": [
        "## Time to first token (TTFT)\n",
        "\n",
        "ストリーミングを使う最大の理由は、**TTFT（Time to first token）** を改善することです。これは、あなた（またはユーザー）がモデル生成の最初の内容を受け取るまでの時間を指します。\n",
        "\n",
        "ストリーミングが TTFT に与える影響をデモしてみましょう。\n",
        "\n",
        "まずは非ストリーミングです。長い文章を生成させますが、`max_tokens=500` で打ち切ります：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90362b43",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "def measure_non_streaming_ttft():\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.messages.create(\n",
        "        max_tokens=500,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "    )\n",
        "\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Time to receive first token: {response_time:.3f} seconds\")\n",
        "    print(f\"Time to recieve complete response: {response_time:.3f} seconds\")\n",
        "    print(f\"Total tokens generated: {response.usage.output_tokens}\")\n",
        "    \n",
        "    print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bfcc324",
      "metadata": {},
      "outputs": [],
      "source": [
        "measure_non_streaming_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d0df49",
      "metadata": {},
      "source": [
        "次に、同じことをストリーミングでやってみます：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820ef396",
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_streaming_ttft():\n",
        "    start_time = time.time()\n",
        "\n",
        "    stream = client.messages.create(\n",
        "        max_tokens=500,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        stream=True\n",
        "    )\n",
        "    have_received_first_token = False\n",
        "    for event in stream:\n",
        "        if event.type == \"content_block_delta\":\n",
        "            if not have_received_first_token:\n",
        "                ttft = time.time() - start_time\n",
        "                have_received_first_token = True\n",
        "            print(event.delta.text, flush=True, end=\"\")\n",
        "        elif event.type == \"message_delta\":\n",
        "            output_tokens = event.usage.output_tokens\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTime to receive first token: {ttft:.3f} seconds\", flush=True)\n",
        "    print(f\"Time to recieve complete response: {total_time:.3f} seconds\", flush=True)\n",
        "    print(f\"Total tokens generated: {output_tokens}\", flush=True)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a84ca9",
      "metadata": {},
      "outputs": [],
      "source": [
        "measure_streaming_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81ed684",
      "metadata": {},
      "source": [
        "結果を比較してみましょう。\n",
        "\n",
        "* **ストリーミングなし**\n",
        "  * **最初のトークンを受け取るまで:** 4.194 秒\n",
        "  * **応答全体を受け取るまで:** 4.194 秒\n",
        "  * **生成トークン数:** 500\n",
        "* **ストリーミングあり**\n",
        "  * **最初のトークンを受け取るまで:** 0.492 秒\n",
        "  * **応答全体を受け取るまで:** 4.274 秒\n",
        "  * **生成トークン数:** 500\n",
        "\n",
        "見ての通り、TTFT に大きな差があります。このデモは 500 トークンで、しかも最速モデルの Haiku を使っています。もし Opus で 1000 トークン生成する例にすると、数値はさらに大きく変わります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75676e74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_ttft():\n",
        "    def measure_streaming_ttft():\n",
        "        start_time = time.time()\n",
        "\n",
        "        stream = client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            stream=True\n",
        "        )\n",
        "        have_received_first_token = False\n",
        "        for event in stream:\n",
        "            if event.type == \"content_block_delta\":\n",
        "                if not have_received_first_token:\n",
        "                    ttft = time.time() - start_time\n",
        "                    have_received_first_token = True\n",
        "            elif event.type == \"message_delta\":\n",
        "                output_tokens = event.usage.output_tokens\n",
        "                total_time = time.time() - start_time\n",
        "        return (ttft, output_tokens)\n",
        "    \n",
        "    def measure_non_streaming_ttft():\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            model=\"claude-3-opus-20240229\"\n",
        "        )\n",
        "        ttft = time.time() - start_time\n",
        "        return (ttft, response.usage.output_tokens)\n",
        "    \n",
        "    streaming_ttft, streaming_tokens = measure_streaming_ttft()\n",
        "    non_streaming_ttft, non_streaming_tokens = measure_non_streaming_ttft()\n",
        "\n",
        "    print(\"OPUS STREAMING\")\n",
        "    print(f\"Time to first token: {streaming_ttft}\")\n",
        "    print(f\"Tokens generated: {streaming_tokens}\")\n",
        "    print(\"#########################################################\")\n",
        "    print(\"OPUS NON STREAMING\")\n",
        "    print(f\"Time to first token: {non_streaming_ttft}\")\n",
        "    print(f\"Tokens generated: {non_streaming_tokens}\")\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ccbbce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT RUN THIS! It takes over a minute to run and generates around 2000 tokens with Opus! \n",
        "compare_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04a31d6",
      "metadata": {},
      "source": [
        "Opus でより長い文章を生成すると、ストリーミングの TTFT 改善はさらに分かりやすくなります。非ストリーミングでは最初のトークンを受け取るまで 47 秒かかりましたが、ストリーミングでは 1.8 秒で受け取れました。\n",
        "\n",
        "**注:** ストリーミングはモデルの総生成時間を魔法のように短縮するわけではありません。最初のデータは早く届きますが、リクエスト開始から最後のトークン受信までの総時間が劇的に短くなるわけではありません。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d858c228",
      "metadata": {},
      "source": [
        "## ストリーミング用ヘルパー\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f1f314",
      "metadata": {},
      "source": [
        "Python SDK には、ストリーミングを扱うための便利機能がいくつかあります。`client.messages.create(stream=True)` の代わりに `client.messages.stream()` を使うと、便利なヘルパーメソッドにアクセスできます。`client.messages.stream()` は `MessageStreamManager`（コンテキストマネージャ）を返し、そこからイベントを emit しつつメッセージを蓄積する `MessageStream` を扱えます。\n",
        "\n",
        "次の例では `client.messages.stream` を使い、`stream.text_stream` で「テキスト delta だけ」を簡単に取り出して表示します。イベント種別を毎回チェックする必要がありません。\n",
        "\n",
        "さらに `get_final_message` という便利メソッドもあり、ストリームを最後まで読み終えた後に「最終的な蓄積済みメッセージ」を返してくれます。ストリーミング表示をしつつ、最終的に完成した全文も必要なときに便利です。\n",
        "\n",
        "次の例は、届いたテキストを順次表示しつつ、完了後に最終メッセージも表示します：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a377ebd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "async def streaming_with_helpers():\n",
        "    async with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write me sonnet about orchids\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "    ) as stream:\n",
        "        async for text in stream.text_stream:\n",
        "            print(text, end=\"\", flush=True)\n",
        "\n",
        "    final_message = await stream.get_final_message()\n",
        "    print(\"\\n\\nSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \")\n",
        "    print(final_message.to_json())\n",
        "\n",
        "await streaming_with_helpers()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97eafbd",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "`client.messages.stream()` を使う場合、任意のストリームイベント発生時や、テキスト生成時などに実行される **カスタムイベントハンドラ** も定義できます。\n",
        "\n",
        "次の例では 2 つのイベントハンドラを使います。モデルに \"generate a 5-word poem\" と頼み、独自の `MyStream` クラスに次のハンドラを定義します：\n",
        "\n",
        "* `on_text` - テキスト ContentBlock が蓄積されるときに呼ばれます。第1引数は text delta、第2引数は現在の蓄積テキストです。この例では、生成テキストをストリームに合わせて表示します（見やすさのため緑色）。\n",
        "* `on_stream_event` - API からイベントを受け取るたびに呼ばれます。この例では、イベント type を出力します。\n",
        "\n",
        "そして `client.messages.stream(..., event_handler=MyStream)` のように渡して、コールバックを登録します：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ef03c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from anthropic import AsyncAnthropic, AsyncMessageStream\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "green = '\\033[32m'\n",
        "reset = '\\033[0m'\n",
        "\n",
        "class MyStream(AsyncMessageStream):\n",
        "    async def on_text(self, text, snapshot):\n",
        "        # This runs only on text delta stream messages\n",
        "        print(green + text + reset, flush=True) #model generated content is printed in green\n",
        "\n",
        "    async def on_stream_event(self, event):\n",
        "        # This runs on any stream event\n",
        "        print(\"on_event fired:\", event.type)\n",
        "\n",
        "async def streaming_events_demo():\n",
        "    async with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Generate a 5-word poem\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        event_handler=MyStream,\n",
        "    ) as stream:\n",
        "        # Get the final accumulated message, after the stream is exhausted\n",
        "        message = await stream.get_final_message()\n",
        "        print(\"accumulated final message: \", message.to_json())\n",
        "\n",
        "await streaming_events_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff205093",
      "metadata": {},
      "source": [
        "Python SDK では、他にも次のようなイベントハンドラを利用できます：\n",
        "\n",
        "##### `on_message(message: Message)`\n",
        "完全な Message オブジェクトが蓄積されたときに発火します（message_stop SSE に対応）。\n",
        "\n",
        "##### `on_content_block(content_block: ContentBlock)`\n",
        "完全な ContentBlock が蓄積されたときに発火します（content_block_stop SSE に対応）。\n",
        "\n",
        "##### `on_exception(exception: Exception)`\n",
        "ストリーミング中に例外が発生したときに発火します。\n",
        "\n",
        "##### `on_timeout()`\n",
        "リクエストがタイムアウトしたときに発火します。\n",
        "\n",
        "##### `on_end()`\n",
        "ストリームで最後に発火するイベントです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efbdae51",
      "metadata": {},
      "source": [
        "***\n",
        "\n",
        "## 演習\n",
        "\n",
        "ストリーミングを使うシンプルな Claude チャットボットを書いてください。次の GIF が動作イメージです。出力の色分けは完全に任意で、GIF を見やすくするためのものです：\n",
        "\n",
        "![streaming_chat_exercise](images/streaming_chat_exercise.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272bea60",
      "metadata": {},
      "source": [
        "### 解答例\n",
        "上の演習の 1 つの実装例です。できればこのノートブックのセルとしてではなく、単体の Python スクリプトとして実行するのがおすすめです：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b212dc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "# Initialize the Anthropic client\n",
        "client = Anthropic()\n",
        "\n",
        "# ANSI color codes\n",
        "BLUE = \"\\033[94m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def chat_with_claude():\n",
        "    print(\"Welcome to the Claude Chatbot!\")\n",
        "    print(\"Type 'quit' to exit the chat.\")\n",
        "    \n",
        "    conversation = []\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(f\"{BLUE}You: {RESET}\")\n",
        "        \n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "        \n",
        "        print(f\"{GREEN}Claude: {RESET}\", end=\"\", flush=True)\n",
        "        \n",
        "        stream = client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",\n",
        "            max_tokens=1000,\n",
        "            messages=conversation,\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        assistant_response = \"\"\n",
        "        for chunk in stream:\n",
        "            if chunk.type == \"content_block_delta\":\n",
        "                content = chunk.delta.text\n",
        "                print(f\"{GREEN}{content}{RESET}\", end=\"\", flush=True)\n",
        "                assistant_response += content\n",
        "        \n",
        "        print()  # New line after the complete response\n",
        "        \n",
        "        conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_with_claude()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d7f95b",
      "metadata": {},
      "source": [
        "***\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
