{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e331b3b8-9bda-43b3-a03f-b2e9005765d9",
      "metadata": {},
      "source": [
        "# ストリーミング\n",
        "\n",
        "## 学習目標\n",
        "\n",
        "* ストリーミングの仕組みを理解する\n",
        "* ストリームイベントを扱えるようになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3536ce73",
      "metadata": {},
      "source": [
        "`anthropic` SDK を import して、クライアントをセットアップしましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a681584d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from anthropic import Anthropic\n",
        "\n",
        "#load environment variable\n",
        "load_dotenv()\n",
        "\n",
        "#automatically looks for an \"ANTHROPIC_API_KEY\" environment variable\n",
        "client = Anthropic()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217ecef5",
      "metadata": {},
      "source": [
        "ここまで私たちは、次のような書き方で Claude にメッセージを送ってきました：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c558c0c2",
      "metadata": {},
      "source": [
        "response = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me an essay about macaws and clay licks in the Amazon\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=800,\n",
        "    temperature=0,\n",
        ")\n",
        "print(\"We have a response back!\")\n",
        "print(\"========================\")\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "470f37aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have a response back!\n",
            "========================\n",
            "Here is an essay about macaws and clay licks in the Amazon:\n",
            "\n",
            "Macaws and Clay Licks in the Amazon\n",
            "\n",
            "Deep within the lush, verdant rainforests of the Amazon basin, a remarkable natural phenomenon takes place. Flocks of brilliantly colored macaws, their vibrant plumage shimmering in the dappled sunlight, gather at special sites known as clay licks. These unique geological formations play a vital role in the lives of these magnificent birds, providing them with essential nutrients and minerals that are crucial for their survival and well-being.\n",
            "\n",
            "Macaws, with their striking red, blue, yellow, and green feathers, are among the most iconic and captivating inhabitants of the Amazon. These large parrots are known for their impressive size, their raucous calls, and their remarkable intelligence. They are highly social creatures, often seen soaring through the canopy in noisy, chattering groups. But it is at the clay licks where these birds truly come into their own, gathering in large numbers to partake in a behavior that is both fascinating and essential to their existence.\n",
            "\n",
            "The clay licks, or \"collpas\" as they are known locally, are natural deposits of mineral-rich clay that are exposed along the banks of rivers and streams. These clay formations have been formed over thousands of years, the result of geological processes that have brought these essential nutrients to the surface. For the macaws, these clay licks represent a vital source of sodium, calcium, and other minerals that are often lacking in their predominantly plant-based diet.\n",
            "\n",
            "As the sun rises over the Amazon, the macaws begin to gather at the clay licks, often in large, boisterous flocks. They land on the exposed clay banks, carefully selecting the most nutrient-rich areas and methodically consuming the clay. This behavior, known as geophagy, is believed to serve several important functions. The clay helps to neutralize toxins and acids that the macaws may have ingested from the various plants and fruits they consume, and it also provides them with essential minerals that are crucial for their overall health and well-being.\n",
            "\n",
            "The sight of these magnificent birds, their vibrant colors contrasting with the earthy tones of the clay, is a truly breathtaking spectacle. Visitors to the Amazon who are lucky enough to witness this natural wonder are often left in awe, captivated by the sheer beauty and grace of the macaws as they go about their daily ritual.\n",
            "\n",
            "But the clay licks of the Amazon are not just a source of fascination for human observers. They are also a vital part of the delicate ecosystem that sustains the diverse array of life found in this remarkable region. The macaws, in their role as seed dispersers and pollinators, play a crucial part in maintaining the health and diversity of the rainforest. By visiting the clay licks, they help to ensure the continued vitality of the plants and trees that are the foundation of this complex and fragile environment.\n",
            "\n",
            "As we continue to grapple with the challenges of preserving the Amazon rainforest, the clay licks and the macaws that depend on them serve as a powerful reminder of the interconnectedness of all life on our planet. By protecting these unique natural wonders, we not only safeguard the future of the macaws, but also the entire web of life that makes the Amazon one of the most remarkable and irreplaceable ecosystems on Earth.\n"
          ]
        }
      ],
      "source": [
        "response = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me an essay about macaws and clay licks in the Amazon\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=800,\n",
        "    temperature=0,\n",
        ")\n",
        "print(\"We have a response back!\")\n",
        "print(\"========================\")\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715293a1",
      "metadata": {},
      "source": [
        "この方法でも問題なく動きますが、このやり方では **すべての内容が生成し終わってから** はじめて API から内容が返ってくる点に注意してください。上のセルをもう一度実行すると、全体の応答が一括で表示されるまで何も出力されないはずです。\n",
        "\n",
        "多くの場面ではこれで十分ですが、ユーザーが「応答が全部生成されるまで待たされる」アプリを作っている場合、体験が悪くなりやすいです。\n",
        "\n",
        "**そこでストリーミングです！**\n",
        "\n",
        "ストリーミングを使うと、モデルが生成した内容を「生成され次第」受け取れるため、全体が完成するのを待つ必要がありません。`claude.ai` のようなアプリはこの仕組みで、生成された内容がブラウザへ順次送られて表示されます：\n",
        "\n",
        "![claude_streaming](images/claude_streaming.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e900e5d2",
      "metadata": {},
      "source": [
        "## ストリームを扱う\n",
        "\n",
        "API からストリーミング応答を得るには、`client.messages.create` に `stream=True` を渡すだけです。ここまでは簡単です。少しややこしいのは、その後にストリーミング応答をどう扱い、流れてくるデータをどう処理するかです。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e53041a",
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7ea405",
      "metadata": {},
      "source": [
        "`stream` 変数の中身を見てみましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf731712",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<anthropic.Stream at 0x714acba4dd30>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stream"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db973d7e",
      "metadata": {},
      "source": [
        "見た目にはあまり情報がありません！この stream オブジェクト単体では、ほとんど何もしてくれません。stream はジェネレータで、API から受け取った server-sent events (SSE) を 1 つずつ yield します。\n",
        "\n",
        "つまり、こちら側で反復（iterate）して、各 SSE を処理するコードを書く必要があります。もう「完成した 1 つの塊」としてデータが返ってくるのではありません。では実際に stream を回してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2446578",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RawMessageStartEvent(message=Message(id='msg_01W8XFY58p4frrKQQca8X8yj', content=[], model='claude-3-haiku-20240307', role='assistant', stop_reason=None, stop_sequence=None, type='message', usage=Usage(cache_creation=CacheCreation(ephemeral_1h_input_tokens=0, ephemeral_5m_input_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=30, output_tokens=2, server_tool_use=None, service_tier='standard')), type='message_start')\n",
            "RawContentBlockStartEvent(content_block=TextBlock(citations=None, text='', type='text'), index=0, type='content_block_start')\n",
            "RawContentBlockDeltaEvent(delta=TextDelta(text='Cats', type='text_delta'), index=0, type='content_block_delta')\n",
            "RawContentBlockDeltaEvent(delta=TextDelta(text=' me', type='text_delta'), index=0, type='content_block_delta')\n",
            "RawContentBlockDeltaEvent(delta=TextDelta(text='ow lou', type='text_delta'), index=0, type='content_block_delta')\n",
            "RawContentBlockDeltaEvent(delta=TextDelta(text='dly.', type='text_delta'), index=0, type='content_block_delta')\n",
            "RawContentBlockStopEvent(index=0, type='content_block_stop')\n",
            "RawMessageDeltaEvent(delta=Delta(stop_reason='end_turn', stop_sequence=None), type='message_delta', usage=MessageDeltaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=30, output_tokens=10, server_tool_use=None))\n",
            "RawMessageStopEvent(type='message_stop')\n"
          ]
        }
      ],
      "source": [
        "for event in stream:\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33eb9f8b",
      "metadata": {},
      "source": [
        "ご覧の通り、API からは多くの SSE（イベント）が届きます。これらの意味をもう少し詳しく見ていきます。次はイベントを色分けして説明した図です：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0929c678",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "![streaming_output](images/streaming_output.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeba33c0",
      "metadata": {},
      "source": [
        "各ストリームは、次の順序でイベントが流れてきます：\n",
        "\n",
        "* **MessageStartEvent** - content が空の Message\n",
        "* **複数の content block**（各ブロックは次を含む）\n",
        "  * **ContentBlockStartEvent**\n",
        "  * 1 個以上の **ContentBlockDeltaEvent**\n",
        "  * **ContentBlockStopEvent**\n",
        "* 最終メッセージのトップレベルの変化を示す **MessageDeltaEvent**（1個以上）\n",
        "* 最後に **MessageStopEvent**\n",
        "\n",
        "上の例では content block は 1 つだけでした。次の図は、その 1 ブロックに関連するイベント全体を示します：\n",
        "\n",
        "![content_block_streaming](images/content_block_streaming.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e791652f",
      "metadata": {},
      "source": [
        "私たちが本当に欲しい「モデルが生成した本文」は、`ContentBlockDeltaEvent` から届きます（`type` が `\"content_block_delta\"`）。実際のテキストは `delta` の中の `text` にあります。生成されたテキストだけを表示してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c794ca3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cats\n",
            " me\n",
            "ow lou\n",
            "dly.\n"
          ]
        }
      ],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9329b127",
      "metadata": {},
      "source": [
        "テキスト自体は出せていますが、出力が分断されて読みづらいです。Python の `print()` でストリーミングテキストを表示するときは、次の 2 つの引数が便利です：\n",
        "\n",
        "* `end=\"\"`: `print()` はデフォルトで末尾に改行（`\n",
        "`）を付けますが、`end=\"\"` にすると改行しません。次の `print()` が同じ行に続いて表示されます。\n",
        "* `flush=True`: バッファを待たずに即時出力します。ストリーミングで「リアルタイムに表示」したいときに有効です。\n",
        "\n",
        "これを反映してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "70beba80",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cats meow loudly."
          ]
        }
      ],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write me a 3 word sentence, without a preamble.  Just give me 3 words\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=100,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d90d94",
      "metadata": {},
      "source": [
        "これくらい短いテキストだと、ストリーミングの効果が分かりにくいかもしれません。もう少し長いものを生成させてみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "818d04b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "大規模言語モデル(LLM)の動作原理は以下のようになります:\n",
            "\n",
            "1. データ収集:大量のテキストデータ(書籍、ウェブページ、会話記録など)を収集する。\n",
            "\n",
            "2. データ前処理:収集したデータを機械学習モデルに適した形式に変換する。単語の分割、文章の正規化など。\n",
            "\n",
            "3. モデルの構築:深層学習のアーキテクチャ(トランスフォーマーなど)を使ってモデルを構築する。\n",
            "\n",
            "4. 学習:大量のテキストデータを使ってモデルを訓練する。単語の共起関係や文脈を学習する。\n",
            "\n",
            "5. 推論:新しい入力に対して、学習したパターンに基づいて文章を生成したり、質問に答えたりする。\n",
            "\n",
            "6. 微調整:特定のタスクや分野に合わせて、事前学習済みのモデルを微調整することもできる。\n",
            "\n",
            "このように、大規模なデータセットを活用し、深層学習のアーキテクチャを活用することで、LLMは人間の言語処理能力に迫る性能を発揮できるようになっています。ただし、バイアスの問題や安全性の課題など、まだ解決すべき課題も残されています。"
          ]
        }
      ],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            # \"content\": \"How do large language models work?\",\n",
        "            \"content\": \"large language modelsはどのように動作しますか?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221e30f2",
      "metadata": {},
      "source": [
        "まだなら、上のセルを実行してみてください。テキストが少しずつ（増分で）表示されるはずです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229d1b68",
      "metadata": {},
      "source": [
        "ここまで見た通り、`ContentBlockDeltaEvent` にモデルが生成したテキストが入っています。ただし、他のイベントも重要です。例えばトークン使用量を知りたい場合、見るべき場所は 2 つあります：\n",
        "\n",
        "* `MessageStartEvent` に入力（プロンプト）のトークン使用量が入る\n",
        "* `MessageDeltaEvent` に出力トークン数が入る\n",
        "\n",
        "![streaming_tokens](images/streaming_tokens.png)\n",
        "\n",
        "では、上のコードを更新して、入力トークン数と出力トークン数を表示してみましょう：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bdcab292",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MESSAGE START EVENT\n",
            "Input tokens used: 21\n",
            "========================\n",
            "大規模言語モデル(LLM)の動作原理は以下のようになります:\n",
            "\n",
            "1. データ収集:大量のテキストデータ(書籍、ウェブページ、会話記録など)を収集する。\n",
            "\n",
            "2. データ前処理:収集したデータを機械学習モデルに適した形式に変換する。単語の正規化、文章の分割など。\n",
            "\n",
            "3. モデルの構築:深層学習のアーキテクチャ(トランスフォーマーなど)を使ってモデルを構築する。\n",
            "\n",
            "4. 学習:大量のテキストデータを使ってモデルを訓練する。単語の共起関係や文脈を学習し、言語の理解と生成を行えるようになる。\n",
            "\n",
            "5. 推論:学習済みのモデルを使って、入力された文章に対して適切な出力を生成する。単語の予測、文章の生成、質問への回答など。\n",
            "\n",
            "6. 微調整:特定のタスクや分野に合わせて、モデルを追加の学習(fine-tuning)することで性能を向上させる。\n",
            "\n",
            "このように、大規模な学習データと強力な深層学習アーキテクチャを組み合わせることで、LLMは人間の言語理解や生成に迫る性能を発揮できるようになっています。ただし、バイアスの問題や安全性の課題など、まだ解決すべき課題も残されています。\n",
            "========================\n",
            "MESSAGE DELTA EVENT\n",
            "Output tokens used: 428\n"
          ]
        }
      ],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"large language modelsはどのように動作しますか?\",\n",
        "            # \"content\": \"How do large language models work?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"message_start\":\n",
        "        input_tokens = event.message.usage.input_tokens\n",
        "        print(\"MESSAGE START EVENT\", flush=True)\n",
        "        print(f\"Input tokens used: {input_tokens}\", flush=True)\n",
        "        print(\"========================\")\n",
        "    elif event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")\n",
        "    elif event.type == \"message_delta\":\n",
        "        output_tokens = event.usage.output_tokens\n",
        "        print(\"\\n========================\", flush=True)\n",
        "        print(\"MESSAGE DELTA EVENT\", flush=True)\n",
        "        print(f\"Output tokens used: {output_tokens}\", flush=True)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea87e85",
      "metadata": {},
      "source": [
        "### その他のストリーミングイベントタイプ\n",
        "\n",
        "ストリームを扱っていると、他にも次のようなイベントに遭遇することがあります：\n",
        "\n",
        "* **Ping イベント** - ストリームには任意個の ping が含まれ得ます。\n",
        "* **Error イベント** - ストリーム内にエラーイベントが混ざることがあります。例えば高負荷時には `overloaded_error` が返ることがあり、非ストリーミングの場合の HTTP 529 に相当します。\n",
        "\n",
        "エラーイベントの例：\n",
        "\n",
        "```\n",
        "event: error\n",
        "data: {\"type\": \"error\", \"error\": {\"type\": \"overloaded_error\", \"message\": \"Overloaded\"}}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6f271d",
      "metadata": {},
      "source": [
        "## Time to first token (TTFT)\n",
        "\n",
        "ストリーミングを使う最大の理由は、**TTFT（Time to first token）** を改善することです。これは、あなた（またはユーザー）がモデル生成の最初の内容を受け取るまでの時間を指します。\n",
        "\n",
        "ストリーミングが TTFT に与える影響をデモしてみましょう。\n",
        "\n",
        "まずは非ストリーミングです。長い文章を生成させますが、`max_tokens=500` で打ち切ります：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90362b43",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "def measure_non_streaming_ttft():\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.messages.create(\n",
        "        max_tokens=500,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"アメリカの独立戦争の歴史を長文で説明してください\",\n",
        "                # \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "    )\n",
        "\n",
        "    response_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Time to receive first token: {response_time:.3f} seconds\")\n",
        "    print(f\"Time to recieve complete response: {response_time:.3f} seconds\")\n",
        "    print(f\"Total tokens generated: {response.usage.output_tokens}\")\n",
        "    \n",
        "    print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8bfcc324",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to receive first token: 4.205 seconds\n",
            "Time to recieve complete response: 4.205 seconds\n",
            "Total tokens generated: 500\n",
            "アメリカの独立戦争は、1775年から1783年にかけて起こった重要な出来事でした。この戦争は、イギリス植民地であったアメリカ13州がイギリス王国からの独立を勝ち取るために行われたものです。\n",
            "\n",
            "戦争の発端は、1763年のフランス・インド戦争の終結後、イギリス政府がアメリカ植民地に対して課税を強化したことにありました。植民地住民は、「課税には代表がなければならない」という原則に基づき、これに反発しました。1773年のボストン茶会事件では、植民地住民がイギリス東インド会社の茶税に抗議して茶船を襲撃する事件が起こりました。\n",
            "\n",
            "これに対してイギリス政府は強硬な対応をとり、植民地に軍隊を派遣しました。1775年4月、マサチューセッツ州レキシントンとコンコードで、イギリス軍と植民地民兵の間で最初の武力衝突が起こりました。これがいわゆる「レキシントン・コンコードの戦い」です。この戦いを機に、植民地住民は本格的な独立戦争を始めることになりました。\n",
            "\n",
            "独立戦争では、ジョージ・ワシントンを最高司令官とする植民地軍が、イギリス軍と激しい戦闘を繰り広げました。1776年7月4日、13の植民地が「独立宣言」を発表し、正式にイギリスからの独立を宣言しました。その後、フランスやスペインなどが植民地側に加わり、1\n"
          ]
        }
      ],
      "source": [
        "measure_non_streaming_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d0df49",
      "metadata": {},
      "source": [
        "次に、同じことをストリーミングでやってみます：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "820ef396",
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_streaming_ttft():\n",
        "    start_time = time.time()\n",
        "\n",
        "    stream = client.messages.create(\n",
        "        max_tokens=500,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"アメリカの独立戦争の歴史を長文で説明してください\",\n",
        "                # \"content\": \"Write mme a long essay explaining the history of the American Revolution\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        stream=True\n",
        "    )\n",
        "    have_received_first_token = False\n",
        "    for event in stream:\n",
        "        if event.type == \"content_block_delta\":\n",
        "            if not have_received_first_token:\n",
        "                ttft = time.time() - start_time\n",
        "                have_received_first_token = True\n",
        "            print(event.delta.text, flush=True, end=\"\")\n",
        "        elif event.type == \"message_delta\":\n",
        "            output_tokens = event.usage.output_tokens\n",
        "            total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTime to receive first token: {ttft:.3f} seconds\", flush=True)\n",
        "    print(f\"Time to recieve complete response: {total_time:.3f} seconds\", flush=True)\n",
        "    print(f\"Total tokens generated: {output_tokens}\", flush=True)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f0a84ca9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "アメリカの独立戦争は、1775年から1783年にかけて起こった重要な出来事でした。この戦争は、イギリス植民地であったアメリカ13州がイギリス王国からの独立を勝ち取るために行われたものです。\n",
            "\n",
            "戦争の発端は、1763年のフランス・インド戦争の終結後、イギリス政府がアメリカ植民地に対して課税を強化したことにありました。植民地住民は、「課税には代表がなければならない」という原則に基づき、これらの税金に反対しました。1773年のボストン茶会事件では、植民地住民がイギリス東インド会社の茶船に乗り込み、茶葉を海に投げ捨てるという事件が起こりました。\n",
            "\n",
            "これに対してイギリス政府は強硬な対応をとり、ボストン港の封鎖や植民地議会の解散など、植民地住民の反発を招きました。1775年4月、マサチューセッツ州レキシントンとコンコードで、イギリス軍と植民地民兵の間で最初の武力衝突が起こりました。これがいわゆる「レキシントン・コンコードの戦い」で、独立戦争の始まりとなりました。\n",
            "\n",
            "その後、1776年7月4日に独立宣言が採択され、アメリカ合衆国の誕生が宣言されました。独立戦争では、ジョージ・ワシントン将軍率いる植民地軍とイギリス軍の間で激しい戦闘が繰り広げられました。1777年のサラトガの戦い\n",
            "Time to receive first token: 0.429 seconds\n",
            "Time to recieve complete response: 3.832 seconds\n",
            "Total tokens generated: 500\n"
          ]
        }
      ],
      "source": [
        "measure_streaming_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81ed684",
      "metadata": {},
      "source": [
        "結果を比較してみましょう。\n",
        "\n",
        "* **ストリーミングなし**\n",
        "  * **最初のトークンを受け取るまで:** 4.194 秒\n",
        "  * **応答全体を受け取るまで:** 4.194 秒\n",
        "  * **生成トークン数:** 500\n",
        "* **ストリーミングあり**\n",
        "  * **最初のトークンを受け取るまで:** 0.492 秒\n",
        "  * **応答全体を受け取るまで:** 4.274 秒\n",
        "  * **生成トークン数:** 500\n",
        "\n",
        "見ての通り、TTFT に大きな差があります。このデモは 500 トークンで、しかも最速モデルの Haiku を使っています。もし Opus で 1000 トークン生成する例にすると、数値はさらに大きく変わります。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75676e74",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_ttft():\n",
        "    def measure_streaming_ttft():\n",
        "        start_time = time.time()\n",
        "\n",
        "        stream = client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"アメリカの独立戦争の歴史を非常に文字数の多い長文で説明してください\",\n",
        "                    # \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            model=\"claude-opus-4-1-20250805\",\n",
        "            # model=\"claude-3-opus-20240229\",\n",
        "            stream=True\n",
        "        )\n",
        "        have_received_first_token = False\n",
        "        for event in stream:\n",
        "            if event.type == \"content_block_delta\":\n",
        "                if not have_received_first_token:\n",
        "                    ttft = time.time() - start_time\n",
        "                    have_received_first_token = True\n",
        "            elif event.type == \"message_delta\":\n",
        "                output_tokens = event.usage.output_tokens\n",
        "                total_time = time.time() - start_time\n",
        "        return (ttft, output_tokens)\n",
        "    \n",
        "    def measure_non_streaming_ttft():\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Write mme a very very long essay explaining the history of the American Revolution\",\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            model=\"claude-opus-4-1-20250805\"\n",
        "            # model=\"claude-3-opus-20240229\"\n",
        "        )\n",
        "        ttft = time.time() - start_time\n",
        "        return (ttft, response.usage.output_tokens)\n",
        "    \n",
        "    streaming_ttft, streaming_tokens = measure_streaming_ttft()\n",
        "    non_streaming_ttft, non_streaming_tokens = measure_non_streaming_ttft()\n",
        "\n",
        "    print(\"OPUS STREAMING\")\n",
        "    print(f\"Time to first token: {streaming_ttft}\")\n",
        "    print(f\"Tokens generated: {streaming_tokens}\")\n",
        "    print(\"#########################################################\")\n",
        "    print(\"OPUS NON STREAMING\")\n",
        "    print(f\"Time to first token: {non_streaming_ttft}\")\n",
        "    print(f\"Tokens generated: {non_streaming_tokens}\")\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31ccbbce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPUS STREAMING\n",
            "Time to first token: 1.5231685638427734\n",
            "Tokens generated: 1000\n",
            "#########################################################\n",
            "OPUS NON STREAMING\n",
            "Time to first token: 32.76705503463745\n",
            "Tokens generated: 1000\n"
          ]
        }
      ],
      "source": [
        "# DO NOT RUN THIS! It takes over a minute to run and generates around 2000 tokens with Opus! \n",
        "compare_ttft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04a31d6",
      "metadata": {},
      "source": [
        "Opus でより長い文章を生成すると、ストリーミングの TTFT 改善はさらに分かりやすくなります。非ストリーミングでは最初のトークンを受け取るまで 47 秒かかりましたが、ストリーミングでは 1.8 秒で受け取れました。\n",
        "\n",
        "**注:** ストリーミングはモデルの総生成時間を魔法のように短縮するわけではありません。最初のデータは早く届きますが、リクエスト開始から最後のトークン受信までの総時間が劇的に短くなるわけではありません。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d858c228",
      "metadata": {},
      "source": [
        "## ストリーミング用ヘルパー\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f1f314",
      "metadata": {},
      "source": [
        "Python SDK には、ストリーミングを扱うための便利機能がいくつかあります。`client.messages.create(stream=True)` の代わりに `client.messages.stream()` を使うと、便利なヘルパーメソッドにアクセスできます。`client.messages.stream()` は `MessageStreamManager`（コンテキストマネージャ）を返し、そこからイベントを emit しつつメッセージを蓄積する `MessageStream` を扱えます。\n",
        "\n",
        "次の例では `client.messages.stream` を使い、`stream.text_stream` で「テキスト delta だけ」を簡単に取り出して表示します。イベント種別を毎回チェックする必要がありません。\n",
        "\n",
        "さらに `get_final_message` という便利メソッドもあり、ストリームを最後まで読み終えた後に「最終的な蓄積済みメッセージ」を返してくれます。ストリーミング表示をしつつ、最終的に完成した全文も必要なときに便利です。\n",
        "\n",
        "次の例は、届いたテキストを順次表示しつつ、完了後に最終メッセージも表示します：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a377ebd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 蘭に寄せて\n",
            "\n",
            "紫の花弁（はなびら）優雅に開きて\n",
            "朝露に濡れし姿の気高さよ\n",
            "温室の中に静かに息づきて\n",
            "異国の香りを運ぶ風のごと\n",
            "\n",
            "胡蝶蘭の白き羽ばたきは\n",
            "天上の使いか夢の化身か\n",
            "カトレアの赤き情熱は燃えて\n",
            "深き森より届く生命（いのち）かな\n",
            "\n",
            "シンビジウムの黄金なる輝き\n",
            "デンドロビウムの可憐な姿\n",
            "千の種類に千の物語\n",
            "\n",
            "永遠（とわ）に咲き継ぐ美の極致\n",
            "人の心を魅了して止まず\n",
            "蘭よ、気品の花よ、永遠（とこしえ）に\n",
            "\n",
            "STREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \n",
            "{\n",
            "  \"id\": \"msg_01KdWBprhgBjcGv6PQqb5HRP\",\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"citations\": null,\n",
            "      \"text\": \"# 蘭に寄せて\\n\\n紫の花弁（はなびら）優雅に開きて\\n朝露に濡れし姿の気高さよ\\n温室の中に静かに息づきて\\n異国の香りを運ぶ風のごと\\n\\n胡蝶蘭の白き羽ばたきは\\n天上の使いか夢の化身か\\nカトレアの赤き情熱は燃えて\\n深き森より届く生命（いのち）かな\\n\\nシンビジウムの黄金なる輝き\\nデンドロビウムの可憐な姿\\n千の種類に千の物語\\n\\n永遠（とわ）に咲き継ぐ美の極致\\n人の心を魅了して止まず\\n蘭よ、気品の花よ、永遠（とこしえ）に\",\n",
            "      \"type\": \"text\"\n",
            "    }\n",
            "  ],\n",
            "  \"model\": \"claude-opus-4-1-20250805\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"stop_reason\": \"end_turn\",\n",
            "  \"stop_sequence\": null,\n",
            "  \"type\": \"message\",\n",
            "  \"usage\": {\n",
            "    \"cache_creation\": {\n",
            "      \"ephemeral_1h_input_tokens\": 0,\n",
            "      \"ephemeral_5m_input_tokens\": 0\n",
            "    },\n",
            "    \"cache_creation_input_tokens\": 0,\n",
            "    \"cache_read_input_tokens\": 0,\n",
            "    \"input_tokens\": 21,\n",
            "    \"output_tokens\": 244,\n",
            "    \"service_tier\": \"standard\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "async def streaming_with_helpers():\n",
        "    async with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"蘭についてのsonnetを作成してください\",\n",
        "                # \"content\": \"Write me sonnet about orchids\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-opus-4-1-20250805\",\n",
        "    ) as stream:\n",
        "        async for text in stream.text_stream:\n",
        "            print(text, end=\"\", flush=True)\n",
        "\n",
        "    final_message = await stream.get_final_message()\n",
        "    print(\"\\n\\nSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \")\n",
        "    print(final_message.to_json())\n",
        "\n",
        "await streaming_with_helpers()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97eafbd",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "`client.messages.stream()` を使う場合、ストリームイベントを処理するために `async for event in stream:` を使ってイベントを反復処理できます。\n",
        "\n",
        "次の例では、モデルに \"generate a 5-word poem\" と頼み、ストリームからイベントを受け取りながら処理します：\n",
        "\n",
        "* `event.type == \"text\"` - テキスト delta イベントの場合、`event.text` に生成されたテキストが含まれます。この例では、生成テキストを緑色で表示します。\n",
        "* すべてのイベントタイプを `event.type` で確認できます（デバッグ用）。\n",
        "\n",
        "**注意**: 最新のAnthropic SDK（0.75.0以降）では、`event_handler`パラメータはサポートされていません。代わりに、`async for event in stream:`を使ってイベントを処理してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3319eaa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from anthropic import AsyncAnthropic, AsyncMessageStream\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "green = '\\033[32m'\n",
        "reset = '\\033[0m'\n",
        "\n",
        "class MyStream(AsyncMessageStream):\n",
        "    async def on_text(self, text, snapshot):\n",
        "        # This runs only on text delta stream messages\n",
        "        print(green + text + reset, flush=True) #model generated content is printed in green\n",
        "\n",
        "    async def on_stream_event(self, event):\n",
        "        # This runs on any stream event\n",
        "        print(\"on_event fired:\", event.type)\n",
        "\n",
        "async def streaming_events_demo():\n",
        "    async with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Generate a 5-word poem\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-3-opus-20240229\",\n",
        "        event_handler=MyStream,\n",
        "    ) as stream:\n",
        "        # Get the final accumulated message, after the stream is exhausted\n",
        "        message = await stream.get_final_message()\n",
        "        print(\"accumulated final message: \", message.to_json())\n",
        "\n",
        "await streaming_events_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3b29a4",
      "metadata": {},
      "source": [
        "現在のSDKでは AsyncMessageStreamクラスをevent_handlerとして渡す 方式は廃止されている。\n",
        "代わりに async for event in stream: でイベントに応じた処理を記述するスタイル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f6ef03c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mMorning\u001b[0m\u001b[32m coffee\u001b[0m\u001b[32m steams,\u001b[0m\u001b[32m dreams\u001b[0m\u001b[32m fade\u001b[0m\u001b[32m.\u001b[0m\n",
            "accumulated final message:  {\n",
            "  \"id\": \"msg_01AvBVk9wDjUqCxsut2Jhuxd\",\n",
            "  \"content\": [\n",
            "    {\n",
            "      \"citations\": null,\n",
            "      \"text\": \"Morning coffee steams, dreams fade.\",\n",
            "      \"type\": \"text\"\n",
            "    }\n",
            "  ],\n",
            "  \"model\": \"claude-opus-4-1-20250805\",\n",
            "  \"role\": \"assistant\",\n",
            "  \"stop_reason\": \"end_turn\",\n",
            "  \"stop_sequence\": null,\n",
            "  \"type\": \"message\",\n",
            "  \"usage\": {\n",
            "    \"cache_creation\": {\n",
            "      \"ephemeral_1h_input_tokens\": 0,\n",
            "      \"ephemeral_5m_input_tokens\": 0\n",
            "    },\n",
            "    \"cache_creation_input_tokens\": 0,\n",
            "    \"cache_read_input_tokens\": 0,\n",
            "    \"input_tokens\": 14,\n",
            "    \"output_tokens\": 11,\n",
            "    \"service_tier\": \"standard\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "green = '\\033[32m'\n",
        "reset = '\\033[0m'\n",
        "\n",
        "async def streaming_events_demo():\n",
        "    async with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Generate a 5-word poem\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-opus-4-1-20250805\",\n",
        "    ) as stream:\n",
        "        # ストリームイベントを処理する\n",
        "        async for event in stream:\n",
        "            # テキストイベントの場合、緑色で表示\n",
        "            if event.type == \"text\":\n",
        "                print(green + event.text + reset, end=\"\", flush=True)\n",
        "            # その他のイベントタイプを確認したい場合は、以下をコメントアウト\n",
        "            # print(\"on_event fired:\", event.type)\n",
        "        \n",
        "        # ストリームが終了したら、最終メッセージを取得\n",
        "        message = await stream.get_final_message()\n",
        "        print(\"\\naccumulated final message: \", message.to_json())\n",
        "\n",
        "await streaming_events_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff205093",
      "metadata": {},
      "source": [
        "Python SDKでは、`async for event in stream:`を使って、以下のようなイベントタイプを処理できます：\n",
        "\n",
        "##### `event.type == \"message_start\"`\n",
        "メッセージの開始を示すイベントです。`event.message`に初期Messageオブジェクトが含まれます。\n",
        "\n",
        "##### `event.type == \"text\"`\n",
        "テキスト delta イベントです。`event.text`に生成されたテキストチャンクが含まれます。\n",
        "\n",
        "##### `event.type == \"content_block_stop\"`\n",
        "完全な ContentBlock が蓄積されたときに発火します。`event.content_block`にContentBlockオブジェクトが含まれます。\n",
        "\n",
        "##### `event.type == \"message_stop\"`\n",
        "メッセージの終了を示すイベントです。`event.message`に完全なMessageオブジェクトが含まれます。\n",
        "\n",
        "##### `event.type == \"message_delta\"`\n",
        "メッセージの更新を示すイベントです。`event.usage`にトークン使用量などの情報が含まれます。\n",
        "\n",
        "すべてのイベントタイプは、`async for event in stream:`ループ内で`event.type`をチェックして処理できます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efbdae51",
      "metadata": {},
      "source": [
        "***\n",
        "\n",
        "## 演習\n",
        "\n",
        "ストリーミングを使うシンプルな Claude チャットボットを書いてください。次の GIF が動作イメージです。出力の色分けは完全に任意で、GIF を見やすくするためのものです：\n",
        "\n",
        "![streaming_chat_exercise](images/streaming_chat_exercise.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272bea60",
      "metadata": {},
      "source": [
        "### 解答例\n",
        "上の演習の 1 つの実装例です。できればこのノートブックのセルとしてではなく、単体の Python スクリプトとして実行するのがおすすめです：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b212dc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Claude Chatbot!\n",
            "Type 'quit' to exit the chat.\n"
          ]
        }
      ],
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "# Initialize the Anthropic client\n",
        "client = Anthropic()\n",
        "\n",
        "# ANSI color codes\n",
        "BLUE = \"\\033[94m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def chat_with_claude():\n",
        "    print(\"Welcome to the Claude Chatbot!\")\n",
        "    print(\"Type 'quit' to exit the chat.\")\n",
        "    \n",
        "    conversation = []\n",
        "    \n",
        "    while True:\n",
        "        user_input = input(f\"{BLUE}You: {RESET}\")\n",
        "        \n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "        \n",
        "        print(f\"{GREEN}Claude: {RESET}\", end=\"\", flush=True)\n",
        "        \n",
        "        stream = client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",\n",
        "            max_tokens=1000,\n",
        "            messages=conversation,\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        assistant_response = \"\"\n",
        "        for chunk in stream:\n",
        "            if chunk.type == \"content_block_delta\":\n",
        "                content = chunk.delta.text\n",
        "                print(f\"{GREEN}{content}{RESET}\", end=\"\", flush=True)\n",
        "                assistant_response += content\n",
        "        \n",
        "        print()  # New line after the complete response\n",
        "        \n",
        "        conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat_with_claude()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d7f95b",
      "metadata": {},
      "source": [
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9675b2f5",
      "metadata": {},
      "source": [
        "## ここから独自の実験\n",
        "非常に長い文章のストリーミングテスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "798d9575",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MESSAGE START EVENT\n",
            "Input tokens used: 38\n",
            "========================\n",
            "# アメリカ独立戦争：自由と独立への長き道のり\n",
            "\n",
            "## 第1章：独立への胎動 - 植民地時代の背景と緊張の高まり\n",
            "\n",
            "### 植民地アメリカの形成と発展\n",
            "\n",
            "17世紀初頭から18世紀中葉にかけて、北アメリカ大陸の東海岸には13のイギリス植民地が形成されていました。1607年のバージニア植民地建設を皮切りに、マサチューセッツ、ニューヨーク、ペンシルベニア、メリーランド、コネチカット、ロードアイランド、デラウェア、ノースカロライナ、サウスカロライナ、ニュージャージー、ニューハンプシャー、ジョージアと、それぞれ異なる経緯と目的を持って建設された植民地は、18世紀中頃までに人口約200万人を擁する一大植民地群へと成長していました。\n",
            "\n",
            "これらの植民地は、地理的条件や建設の経緯により、大きく三つの地域に分類されていました。ニューイングランド植民地（マサチューセッツ、コネチカット、ロードアイランド、ニューハンプシャー）は、ピューリタンを中心とした宗教的動機による入植者が多く、小規模農業、漁業、造船業、商業が発達していました。中部植民地（ニューヨーク、ペンシルベニア、ニュージャージー、デラウェア）は、多様な民族と宗教が混在し、穀物生産と商業の中心地として栄えていました。南部植民地（バージニア、メリーランド、ノースカロライナ、サウスカロライナ、ジョージア）は、タバコ、米、藍などのプランテーション農業が主体で、奴隷制度に依存した経済構造を持っていました。\n",
            "\n",
            "### 七年戦争とその影響\n",
            "\n",
            "1756年から1763年にかけて戦われた七年戦争（アメリカではフレンチ・インディアン戦争と呼ばれる）は、イギリスとフランスの間で北アメリカの覇権を巡って争われた戦争でした。この戦争でイギリスは勝利を収め、1763年のパリ条約によってフランスから広大な北アメリカ領土を獲得しました。しかし、この勝利は同時に巨額の戦費による財政危機をもたらし、イギリス政府は植民地への課税強化によってこの負債を解消しようと考えるようになりました。\n",
            "\n",
            "さらに、1763年10月7日に発布された「1763年宣言」は、アパラチア山脈以西への植民地人の入植を禁止し、この地域を先住民居留地として保護することを定めました。これは先住民との紛争を避け、統治コストを削減するための措置でしたが、西部への拡張を望んでいた植民地人にとっては大きな不満の種となりました。\n",
            "\n",
            "### 課税政策と植民地の反発\n",
            "\n",
            "イギリス政府は戦後の財政再建のため、一連の課税法を制定しました。1764年の砂糖法は、西インド諸島からの糖蜜輸入に対する関税\n",
            "========================\n",
            "MESSAGE DELTA EVENT\n",
            "Output tokens used: 1000\n"
          ]
        }
      ],
      "source": [
        "stream = client.messages.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"アメリカの独立戦争の歴史を非常に文字数の多い長文で説明してください\",\n",
        "            # \"content\": \"How do large language models work?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-opus-4-1-20250805\",\n",
        "    max_tokens=1000,\n",
        "    temperature=0,\n",
        "    stream=True,\n",
        ")\n",
        "for event in stream:\n",
        "    if event.type == \"message_start\":\n",
        "        input_tokens = event.message.usage.input_tokens\n",
        "        print(\"MESSAGE START EVENT\", flush=True)\n",
        "        print(f\"Input tokens used: {input_tokens}\", flush=True)\n",
        "        print(\"========================\")\n",
        "    elif event.type == \"content_block_delta\":\n",
        "        print(event.delta.text, flush=True, end=\"\")\n",
        "    elif event.type == \"message_delta\":\n",
        "        output_tokens = event.usage.output_tokens\n",
        "        print(\"\\n========================\", flush=True)\n",
        "        print(\"MESSAGE DELTA EVENT\", flush=True)\n",
        "        print(f\"Output tokens used: {output_tokens}\", flush=True)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb88f9b0",
      "metadata": {},
      "source": [
        "ストリーミングを非同期ではなく実行するとどうなるか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "da758767",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_46498/2716704181.py:6: RuntimeWarning: coroutine 'AsyncAPIClient.post' was never awaited\n",
            "  with client.messages.stream(\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'AsyncMessageStreamManager' object does not support the context manager protocol",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(final_message.to_json())\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mstreaming_with_helpers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mstreaming_with_helpers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstreaming_with_helpers\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m蘭についてのsonnetを作成してください\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# \"content\": \"Write me sonnet about orchids\",\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaude-opus-4-1-20250805\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: 'AsyncMessageStreamManager' object does not support the context manager protocol"
          ]
        }
      ],
      "source": [
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "client = AsyncAnthropic()\n",
        "\n",
        "def streaming_with_helpers():\n",
        "    with client.messages.stream(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"蘭についてのsonnetを作成してください\",\n",
        "                # \"content\": \"Write me sonnet about orchids\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-opus-4-1-20250805\",\n",
        "    ) as stream:\n",
        "        for text in stream.text_stream:\n",
        "            print(text, end=\"\", flush=True)\n",
        "\n",
        "    final_message = stream.get_final_message()\n",
        "    print(\"\\n\\nSTREAMING IS DONE.  HERE IS THE FINAL ACCUMULATED MESSAGE: \")\n",
        "    print(final_message.to_json())\n",
        "\n",
        "streaming_with_helpers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19db110a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
